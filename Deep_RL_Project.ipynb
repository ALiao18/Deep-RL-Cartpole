{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b90d9d4b-7ba8-4c06-87a7-6522c5ebf4c3",
      "metadata": {
        "id": "b90d9d4b-7ba8-4c06-87a7-6522c5ebf4c3"
      },
      "source": [
        "# Reinforcement learning project\n",
        "\n",
        "Contact: Elie KADOCHE, eliekadoche78@gmail.com\n",
        "\n",
        "This is a reinforcement learning project on policy gradient methods.\n",
        "In this Jupyter notebook you will find all the code and instructions.\n",
        "More explanations will be given in the board during the practical sessions.\n",
        "Do not hesitate to ask for help if you need to.\n",
        "The overall project will be noted on 20.\n",
        "The notation is given for information and may slightly change.\n",
        "Your solutions should all appear in this Jupyter notebook, with both: explanations and code.\n",
        "- You will write your answers, thoughts and approaches for each Section.\n",
        "Even if you do some mistakes, you can describe them and explain how you corrected them.\n",
        "It will be taken into account in the notation.\n",
        "- Each time there is a comment `# ---> TODO:`, there is some code missing that you need to write.\n",
        "Be mindful of the code quality.\n",
        "Each line of code should be associated to a comment where you describe what you do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86dfaf76-5293-474b-a6ba-ed24def01994",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86dfaf76-5293-474b-a6ba-ed24def01994",
        "outputId": "3f1cc6f6-f48f-404c-df2e-9c6828b28122",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a70c3c-2338-44d3-9bd6-db9376d0a0b7",
      "metadata": {
        "id": "41a70c3c-2338-44d3-9bd6-db9376d0a0b7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from gymnasium import spaces\n",
        "from gymnasium.utils import seeding\n",
        "from numpy.random import default_rng\n",
        "from torch import nn as nn\n",
        "from torch import optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c228e5-9b95-4c9a-87df-b7f2dd475399",
      "metadata": {
        "id": "48c228e5-9b95-4c9a-87df-b7f2dd475399"
      },
      "source": [
        "## 1) Cartpole environment (/2)\n",
        "\n",
        "Your objective is to understand how the cartpole environment works and what is the problem we want to solve.\n",
        "You do not need to write any code for this Section.\n",
        "You need to write what are the states, the rewards and the actions.\n",
        "Write down the Markov Decision Process associated to the problem.\n",
        "You can read the Section 5 of this [document](http://incompleteideas.net/papers/barto-sutton-anderson-83.pdf) for help.\n",
        "Below are: the environment and a script to test a random policy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39629d9-81a5-4122-ab8c-6f2aee2c7066",
      "metadata": {
        "id": "e39629d9-81a5-4122-ab8c-6f2aee2c7066"
      },
      "source": [
        "**State:**  \n",
        "x - position of the cart on the track,  \n",
        "θ - angle of the pole with the vertical,  \n",
        "ẋ - cart velocity, and  \n",
        "θ̇ - rate of change of the angle.\n",
        "\n",
        "**Action**:     \n",
        "The action in a 2D case is the cart either moving right or left\n",
        "\n",
        "**Reward:**   \n",
        "Whenever the failure signal shows, the reward is subtracted.\n",
        "\n",
        "**Markov Decision Process**.   \n",
        "The MDP takes in the previous state and takes a certain action through a policy to obtain the probability of the next state given the current state and action. The MDP only takes into account the current state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9174e22d-a6a3-4e15-94f2-5f203eda2771",
      "metadata": {
        "id": "9174e22d-a6a3-4e15-94f2-5f203eda2771",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\"\"\"Cartpole environment (default).\"\"\"\n",
        "\n",
        "class CartpoleEnvV0(gym.Env):\n",
        "    \"\"\"Cartpole environment.\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
        "\n",
        "    def __init__(self, env_context=None, render_mode=None):\n",
        "        \"\"\"Initialize environment.\n",
        "\n",
        "        Args:\n",
        "            env_context (dict): environment configuration.\n",
        "            render_mode (str): render mode.\n",
        "        \"\"\"\n",
        "        # Variables\n",
        "        self.gravity = 9.8\n",
        "        self.masscart = 1.0\n",
        "        self.masspole = 0.1\n",
        "        self.total_mass = self.masspole + self.masscart\n",
        "        self.length = 0.5  # Actually half the pole's length\n",
        "        self.polemass_length = self.masspole * self.length\n",
        "        self.force_mag = 10.0\n",
        "        self.tau = 0.02  # Seconds between state updates\n",
        "        self.kinematics_integrator = \"euler\"\n",
        "\n",
        "        # Angle at which to fail the episode\n",
        "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
        "        self.x_threshold = 2.4\n",
        "\n",
        "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
        "        # is still within bounds\n",
        "        high = np.array([\n",
        "            self.x_threshold * 2,\n",
        "            np.finfo(np.float32).max,\n",
        "            self.theta_threshold_radians * 2,\n",
        "            np.finfo(np.float32).max,\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # Action and observation (state) spaces\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
        "\n",
        "        # Render mode\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Others\n",
        "        self.screen_width = 600\n",
        "        self.screen_height = 400\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "        self.state = None\n",
        "\n",
        "    def _process_state(self):\n",
        "        \"\"\"Process state before returning it.\n",
        "\n",
        "        Returns:\n",
        "            state_processed (numpy.array): processed state.\n",
        "        \"\"\"\n",
        "        # No modifications\n",
        "        processed_state = self.state\n",
        "\n",
        "        return processed_state\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment.\n",
        "\n",
        "        Args:\n",
        "            seed (int): seed for reproducibility.\n",
        "            options (dict): additional information.\n",
        "\n",
        "        Returns:\n",
        "            state (numpy.array): the processed state.\n",
        "\n",
        "            info (dict): auxiliary diagnostic information.\n",
        "        \"\"\"\n",
        "        # Reset seed\n",
        "        if seed is not None:\n",
        "            self._np_random, seed = seeding.np_random(seed)\n",
        "\n",
        "        # Current time step\n",
        "        self._time_step = 0\n",
        "\n",
        "        # Reset state\n",
        "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
        "        self.state = self.state.astype(np.float32)\n",
        "\n",
        "        # Eventually render\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return self._process_state(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Go from current step to next one.\n",
        "\n",
        "        Args:\n",
        "            action (int): action of the agent.\n",
        "\n",
        "        Returns:\n",
        "            state (numpy.array): state.\n",
        "\n",
        "            reward (float): reward.\n",
        "\n",
        "            terminated (bool): whether a terminal state is reached.\n",
        "\n",
        "            truncated (bool): whether a truncation condition is reached.\n",
        "\n",
        "            info (dict): auxiliary diagnostic information.\n",
        "        \"\"\"\n",
        "        # Check if action is valid\n",
        "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
        "        assert self.action_space.contains(action), err_msg\n",
        "        assert self.state is not None, \"Call reset before using step method.\"\n",
        "\n",
        "        # Compute variables\n",
        "        x_tmp = self.state\n",
        "        x, x_dot, theta, theta_dot = x_tmp[0], x_tmp[1], x_tmp[2], x_tmp[3]\n",
        "        force = self.force_mag if action == 1 else -self.force_mag\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "\n",
        "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
        "        m = self.polemass_length\n",
        "        temp = force + m * theta_dot**2 * sintheta / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta * temp)\n",
        "        thetaacc /= 4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass\n",
        "        thetaacc /= self.length\n",
        "        xacc = temp - m * thetaacc * costheta / self.total_mass\n",
        "\n",
        "        # Update system\n",
        "        if self.kinematics_integrator == \"euler\":\n",
        "            x = x + self.tau * x_dot\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "        else:  # Semi-implicit euler\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            x = x + self.tau * x_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "\n",
        "        # Full system state\n",
        "        self.state = np.array([\n",
        "            x,\n",
        "            x_dot,\n",
        "            theta,\n",
        "            theta_dot,\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # Reward is 1\n",
        "        reward = 1.0\n",
        "\n",
        "        # Increase time step\n",
        "        self._time_step += 1\n",
        "\n",
        "        # Check if episode if finished\n",
        "        terminated = bool(\n",
        "            x < -self.x_threshold\n",
        "            or x > self.x_threshold\n",
        "            or theta < -self.theta_threshold_radians\n",
        "            or theta > self.theta_threshold_radians\n",
        "            or self._time_step >= 500\n",
        "        )\n",
        "\n",
        "        # Eventually render\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return self._process_state(), reward, terminated, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render environment.\n",
        "\n",
        "        Note:\n",
        "            Do not pay too much attention to this function. It is just to\n",
        "            display a nice animation of the environment.\n",
        "        \"\"\"\n",
        "        import pygame\n",
        "        from pygame import gfxdraw\n",
        "\n",
        "        # Initialize render mode if needed\n",
        "        if self.render_mode is None:\n",
        "            self.render_mode = \"human\"\n",
        "\n",
        "        # Initialize objects\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            if self.render_mode == \"human\":\n",
        "                pygame.display.init()\n",
        "                self.screen = pygame.display.set_mode(\n",
        "                    (self.screen_width, self.screen_height))\n",
        "            else:  # mode == \"rgb_array\"\n",
        "                self.screen = pygame.Surface(\n",
        "                    (self.screen_width, self.screen_height))\n",
        "\n",
        "        # Initialize clock\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        # Objects\n",
        "        world_width = self.x_threshold * 2\n",
        "        scale = self.screen_width / world_width\n",
        "        polewidth = 10.0\n",
        "        polelen = scale * (2 * self.length)\n",
        "        cartwidth = 50.0\n",
        "        cartheight = 30.0\n",
        "\n",
        "        # Get state\n",
        "        if self.state is None:\n",
        "            return None\n",
        "        x = self.state\n",
        "\n",
        "        # Get surface\n",
        "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
        "        self.surf.fill((255, 255, 255))\n",
        "\n",
        "        # Computations\n",
        "        l = -cartwidth / 2\n",
        "        r = cartwidth / 2\n",
        "        t = cartheight / 2\n",
        "        b = -cartheight / 2\n",
        "        axleoffset = cartheight / 4.0\n",
        "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
        "        carty = 100  # TOP OF CART\n",
        "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
        "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
        "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
        "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
        "\n",
        "        l, r, t, b = (\n",
        "            -polewidth / 2,\n",
        "            polewidth / 2,\n",
        "            polelen - polewidth / 2,\n",
        "            -polewidth / 2,\n",
        "        )\n",
        "\n",
        "        pole_coords = []\n",
        "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
        "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
        "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
        "            pole_coords.append(coord)\n",
        "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
        "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
        "\n",
        "        gfxdraw.aacircle(self.surf,\n",
        "                         int(cartx),\n",
        "                         int(carty + axleoffset),\n",
        "                         int(polewidth / 2),\n",
        "                         (129, 132, 203))\n",
        "        gfxdraw.filled_circle(self.surf,\n",
        "                              int(cartx),\n",
        "                              int(carty + axleoffset),\n",
        "                              int(polewidth / 2),\n",
        "                              (129, 132, 203))\n",
        "\n",
        "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
        "\n",
        "        # Display\n",
        "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
        "        self.screen.blit(self.surf, (0, 0))\n",
        "\n",
        "        # Human mode\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            pygame.display.flip()\n",
        "\n",
        "        # RGB array mode\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)),\n",
        "                axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the environment.\n",
        "\n",
        "        Note:\n",
        "            Do not pay too much attention to this function. It is just to close\n",
        "            the environment.\n",
        "        \"\"\"\n",
        "        if self.screen is not None:\n",
        "            import pygame\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.isopen = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ef0f26-766e-4bdd-bfb0-d1a46819d068",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70ef0f26-766e-4bdd-bfb0-d1a46819d068",
        "outputId": "25be6289-b474-4b1e-ab88-382c3908161b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_reward = 11.0\n"
          ]
        }
      ],
      "source": [
        "# Script to run a random policy\n",
        "# ------------------------------------------\n",
        "\n",
        "# Create environment\n",
        "env = CartpoleEnvV0()\n",
        "\n",
        "# Create random generator\n",
        "generator = default_rng(seed=None)\n",
        "\n",
        "# Reset it\n",
        "total_reward = 0.0\n",
        "state, _ = env.reset(seed=None)\n",
        "\n",
        "# While the episode is not finished\n",
        "terminated = False\n",
        "while not terminated:\n",
        "\n",
        "    # Select a random action\n",
        "    action = generator.integers(0, 2)\n",
        "\n",
        "    # One step forward\n",
        "    state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "    # Render (or not) the environment\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "\n",
        "# Print reward\n",
        "env.close()\n",
        "print(\"total_reward = {}\".format(total_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7f62cc-e068-4c18-8c65-866703c32dba",
      "metadata": {
        "id": "6c7f62cc-e068-4c18-8c65-866703c32dba"
      },
      "source": [
        "## 2) Deep neural network policy (/2)\n",
        "\n",
        "We want to build a policy $\\pi_\\theta(a | s) = P(a | s, \\theta)$ that gives the probability of choosing an action $a$ in state $s$.\n",
        "The policy is a deep neural network parameterized by some weights $\\theta$.\n",
        "The policy is also referred to as \"actor\".\n",
        "Depending on what you understood from the cartpole environment, you need to change in the actor code the `input_size` and `nb_actions` variables.\n",
        "In the code running the policy on cartpole, you need to find how to select an action from the ouptut of the policy.\n",
        "Explain your method.\n",
        "How does the policy performs on cartpole?\n",
        "Why?\n",
        "Below are: an example of a simple neural network in PyTorch, the actor code for Cartpole and a script to run the policy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10548456-b54a-4a7f-94c6-21ee58dce7b0",
      "metadata": {
        "id": "10548456-b54a-4a7f-94c6-21ee58dce7b0"
      },
      "source": [
        "**Your answer**:   \n",
        "We use a policy gradient algorithm to solve the bellman equation to find the optimum policy by performing gradient ascent to maximize the expected outcome for the reward given the initial state. We can do this by the hypothesis that the decision process is a markov decision process. The current method performs poorly, only achieveing a consistent reward under 15. This is because we have not ran the backpropagation algorithm, and the optimizer step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cedbab27-b416-4c16-b88e-7eb158b0cee8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "cedbab27-b416-4c16-b88e-7eb158b0cee8",
        "outputId": "29e2025d-8aa6-429b-bb3d-ab303e26b0ce",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.96E+02\n",
            "3.16E+02\n",
            "3.15E+02\n",
            "2.72E+02\n",
            "2.32E+02\n",
            "2.89E+02\n",
            "2.64E+02\n",
            "2.33E+02\n",
            "2.15E+02\n",
            "2.11E+02\n",
            "1.81E+02\n",
            "1.83E+02\n",
            "1.34E+02\n",
            "1.32E+02\n",
            "1.32E+02\n",
            "1.12E+02\n",
            "8.83E+01\n",
            "6.32E+01\n",
            "3.94E+01\n",
            "3.71E+01\n",
            "3.00E+01\n",
            "2.59E+01\n",
            "2.70E+01\n",
            "3.03E+01\n",
            "3.20E+01\n",
            "3.51E+01\n",
            "3.81E+01\n",
            "3.48E+01\n",
            "3.21E+01\n",
            "2.94E+01\n",
            "2.15E+01\n",
            "1.95E+01\n",
            "1.95E+01\n",
            "2.28E+01\n",
            "2.37E+01\n",
            "2.36E+01\n",
            "2.00E+01\n",
            "2.11E+01\n",
            "1.98E+01\n",
            "2.00E+01\n",
            "1.50E+01\n",
            "1.57E+01\n",
            "1.56E+01\n",
            "1.53E+01\n",
            "1.54E+01\n",
            "1.72E+01\n",
            "1.44E+01\n",
            "1.23E+01\n",
            "1.17E+01\n",
            "1.11E+01\n",
            "1.01E+01\n",
            "1.07E+01\n",
            "1.11E+01\n",
            "1.09E+01\n",
            "9.02E+00\n",
            "8.40E+00\n",
            "7.55E+00\n",
            "6.89E+00\n",
            "7.56E+00\n",
            "7.78E+00\n",
            "7.57E+00\n",
            "6.87E+00\n",
            "7.40E+00\n",
            "6.41E+00\n",
            "6.30E+00\n",
            "7.17E+00\n",
            "6.86E+00\n",
            "6.72E+00\n",
            "6.47E+00\n",
            "6.34E+00\n",
            "5.74E+00\n",
            "6.36E+00\n",
            "6.00E+00\n",
            "5.75E+00\n",
            "6.46E+00\n",
            "6.36E+00\n",
            "5.95E+00\n",
            "5.37E+00\n",
            "5.68E+00\n",
            "6.33E+00\n",
            "5.75E+00\n",
            "5.50E+00\n",
            "5.52E+00\n",
            "5.70E+00\n",
            "5.20E+00\n",
            "5.46E+00\n",
            "5.02E+00\n",
            "5.34E+00\n",
            "4.67E+00\n",
            "5.02E+00\n",
            "4.97E+00\n",
            "4.73E+00\n",
            "4.87E+00\n",
            "4.10E+00\n",
            "4.32E+00\n",
            "4.32E+00\n",
            "4.46E+00\n",
            "4.05E+00\n",
            "4.11E+00\n",
            "3.99E+00\n",
            "3.90E+00\n",
            "4.13E+00\n",
            "3.70E+00\n",
            "3.48E+00\n",
            "3.45E+00\n",
            "3.80E+00\n",
            "3.52E+00\n",
            "3.32E+00\n",
            "3.27E+00\n",
            "3.11E+00\n",
            "3.65E+00\n",
            "3.19E+00\n",
            "3.15E+00\n",
            "2.72E+00\n",
            "3.05E+00\n",
            "3.12E+00\n",
            "2.78E+00\n",
            "2.68E+00\n",
            "2.74E+00\n",
            "2.49E+00\n",
            "2.51E+00\n",
            "2.59E+00\n",
            "2.46E+00\n",
            "2.50E+00\n",
            "2.57E+00\n",
            "2.35E+00\n",
            "2.23E+00\n",
            "2.36E+00\n",
            "2.32E+00\n",
            "2.31E+00\n",
            "2.09E+00\n",
            "2.02E+00\n",
            "2.13E+00\n",
            "2.14E+00\n",
            "2.02E+00\n",
            "2.02E+00\n",
            "1.86E+00\n",
            "1.66E+00\n",
            "1.76E+00\n",
            "1.86E+00\n",
            "1.79E+00\n",
            "1.62E+00\n",
            "1.68E+00\n",
            "1.76E+00\n",
            "1.56E+00\n",
            "1.58E+00\n",
            "1.68E+00\n",
            "1.45E+00\n",
            "1.41E+00\n",
            "1.36E+00\n",
            "1.42E+00\n",
            "1.23E+00\n",
            "1.42E+00\n",
            "1.27E+00\n",
            "1.18E+00\n",
            "1.17E+00\n",
            "1.13E+00\n",
            "1.11E+00\n",
            "1.35E+00\n",
            "9.70E-01\n",
            "1.15E+00\n",
            "9.02E-01\n",
            "9.30E-01\n",
            "1.02E+00\n",
            "9.72E-01\n",
            "8.76E-01\n",
            "9.01E-01\n",
            "8.01E-01\n",
            "9.59E-01\n",
            "7.90E-01\n",
            "7.90E-01\n",
            "7.84E-01\n",
            "7.64E-01\n",
            "6.93E-01\n",
            "7.62E-01\n",
            "6.10E-01\n",
            "7.27E-01\n",
            "7.24E-01\n",
            "6.21E-01\n",
            "5.94E-01\n",
            "5.90E-01\n",
            "6.23E-01\n",
            "5.83E-01\n",
            "5.70E-01\n",
            "5.04E-01\n",
            "5.48E-01\n",
            "5.48E-01\n",
            "4.96E-01\n",
            "4.76E-01\n",
            "4.36E-01\n",
            "4.27E-01\n",
            "4.48E-01\n",
            "4.55E-01\n",
            "4.05E-01\n",
            "3.83E-01\n",
            "4.15E-01\n",
            "3.97E-01\n",
            "3.88E-01\n",
            "3.51E-01\n",
            "3.50E-01\n",
            "3.36E-01\n",
            "3.60E-01\n",
            "2.73E-01\n",
            "3.28E-01\n",
            "3.46E-01\n",
            "3.24E-01\n",
            "3.23E-01\n",
            "3.12E-01\n",
            "3.21E-01\n",
            "2.86E-01\n",
            "3.22E-01\n",
            "2.97E-01\n",
            "3.13E-01\n",
            "3.23E-01\n",
            "2.88E-01\n",
            "2.94E-01\n",
            "2.65E-01\n",
            "3.23E-01\n",
            "2.64E-01\n",
            "2.81E-01\n",
            "2.74E-01\n",
            "2.77E-01\n",
            "2.33E-01\n",
            "2.51E-01\n",
            "2.42E-01\n",
            "2.60E-01\n",
            "2.42E-01\n",
            "2.25E-01\n",
            "2.64E-01\n",
            "2.48E-01\n",
            "2.27E-01\n",
            "2.43E-01\n",
            "2.29E-01\n",
            "2.05E-01\n",
            "2.56E-01\n",
            "2.13E-01\n",
            "2.01E-01\n",
            "1.84E-01\n",
            "2.27E-01\n",
            "2.55E-01\n",
            "2.22E-01\n",
            "1.99E-01\n",
            "2.58E-01\n",
            "2.47E-01\n",
            "2.09E-01\n",
            "1.92E-01\n",
            "2.25E-01\n",
            "2.20E-01\n",
            "2.02E-01\n",
            "1.88E-01\n",
            "2.10E-01\n",
            "2.06E-01\n",
            "1.78E-01\n",
            "1.64E-01\n",
            "1.82E-01\n",
            "2.06E-01\n",
            "1.64E-01\n",
            "1.66E-01\n",
            "1.54E-01\n",
            "1.71E-01\n",
            "1.77E-01\n",
            "1.79E-01\n",
            "1.64E-01\n",
            "1.66E-01\n",
            "1.58E-01\n",
            "1.61E-01\n",
            "1.50E-01\n",
            "1.70E-01\n",
            "1.53E-01\n",
            "1.41E-01\n",
            "1.76E-01\n",
            "1.65E-01\n",
            "1.47E-01\n",
            "1.62E-01\n",
            "1.51E-01\n",
            "1.36E-01\n",
            "1.44E-01\n",
            "1.35E-01\n",
            "1.70E-01\n",
            "1.28E-01\n",
            "1.52E-01\n",
            "1.43E-01\n",
            "1.49E-01\n",
            "1.46E-01\n",
            "1.35E-01\n",
            "1.48E-01\n",
            "1.29E-01\n",
            "1.27E-01\n",
            "1.39E-01\n",
            "1.19E-01\n",
            "1.24E-01\n",
            "1.21E-01\n",
            "1.43E-01\n",
            "1.31E-01\n",
            "1.19E-01\n",
            "1.34E-01\n",
            "1.22E-01\n",
            "1.30E-01\n",
            "1.26E-01\n",
            "1.33E-01\n",
            "1.33E-01\n",
            "1.18E-01\n",
            "1.04E-01\n",
            "1.21E-01\n",
            "1.22E-01\n",
            "1.23E-01\n",
            "1.23E-01\n",
            "1.18E-01\n",
            "1.22E-01\n",
            "1.08E-01\n",
            "1.02E-01\n",
            "1.23E-01\n",
            "1.11E-01\n",
            "1.14E-01\n",
            "8.67E-02\n",
            "1.14E-01\n",
            "1.04E-01\n",
            "1.06E-01\n",
            "1.00E-01\n",
            "1.09E-01\n",
            "1.14E-01\n",
            "1.16E-01\n",
            "1.06E-01\n",
            "9.50E-02\n",
            "9.97E-02\n",
            "9.93E-02\n",
            "1.18E-01\n",
            "8.86E-02\n",
            "9.52E-02\n",
            "1.00E-01\n",
            "8.96E-02\n",
            "1.02E-01\n",
            "9.68E-02\n",
            "1.03E-01\n",
            "9.15E-02\n",
            "8.92E-02\n",
            "1.03E-01\n",
            "9.66E-02\n",
            "7.97E-02\n",
            "8.05E-02\n",
            "9.52E-02\n",
            "7.68E-02\n",
            "9.39E-02\n",
            "7.80E-02\n",
            "7.93E-02\n",
            "8.70E-02\n",
            "7.30E-02\n",
            "7.54E-02\n",
            "6.71E-02\n",
            "7.29E-02\n",
            "7.56E-02\n",
            "7.23E-02\n",
            "7.89E-02\n",
            "8.03E-02\n",
            "6.93E-02\n",
            "7.24E-02\n",
            "7.60E-02\n",
            "7.15E-02\n",
            "7.30E-02\n",
            "7.68E-02\n",
            "7.32E-02\n",
            "6.95E-02\n",
            "7.04E-02\n",
            "7.62E-02\n",
            "6.51E-02\n",
            "6.22E-02\n",
            "6.51E-02\n",
            "5.84E-02\n",
            "5.82E-02\n",
            "6.55E-02\n",
            "6.54E-02\n",
            "6.94E-02\n",
            "6.22E-02\n",
            "6.11E-02\n",
            "5.79E-02\n",
            "6.42E-02\n",
            "6.05E-02\n",
            "5.77E-02\n",
            "5.83E-02\n",
            "5.99E-02\n",
            "6.56E-02\n",
            "6.63E-02\n",
            "5.78E-02\n",
            "5.62E-02\n",
            "4.96E-02\n",
            "5.54E-02\n",
            "6.03E-02\n",
            "4.83E-02\n",
            "5.41E-02\n",
            "5.73E-02\n",
            "5.09E-02\n",
            "5.17E-02\n",
            "5.48E-02\n",
            "5.31E-02\n",
            "4.61E-02\n",
            "5.50E-02\n",
            "5.25E-02\n",
            "4.54E-02\n",
            "5.43E-02\n",
            "4.53E-02\n",
            "4.73E-02\n",
            "5.31E-02\n",
            "5.00E-02\n",
            "4.48E-02\n",
            "3.75E-02\n",
            "5.03E-02\n",
            "4.73E-02\n",
            "4.16E-02\n",
            "4.73E-02\n",
            "4.44E-02\n",
            "4.09E-02\n",
            "4.81E-02\n",
            "4.16E-02\n",
            "4.19E-02\n",
            "3.36E-02\n",
            "4.22E-02\n",
            "3.72E-02\n",
            "3.88E-02\n",
            "4.22E-02\n",
            "3.28E-02\n",
            "3.64E-02\n",
            "3.82E-02\n",
            "3.14E-02\n",
            "3.68E-02\n",
            "3.40E-02\n",
            "3.75E-02\n",
            "2.90E-02\n",
            "3.30E-02\n",
            "3.20E-02\n",
            "3.44E-02\n",
            "3.34E-02\n",
            "3.19E-02\n",
            "3.31E-02\n",
            "3.16E-02\n",
            "3.00E-02\n",
            "3.15E-02\n",
            "3.19E-02\n",
            "2.84E-02\n",
            "2.76E-02\n",
            "3.01E-02\n",
            "2.81E-02\n",
            "2.92E-02\n",
            "2.54E-02\n",
            "2.71E-02\n",
            "2.45E-02\n",
            "2.84E-02\n",
            "2.60E-02\n",
            "2.40E-02\n",
            "2.56E-02\n",
            "2.65E-02\n",
            "2.68E-02\n",
            "2.46E-02\n",
            "3.25E-02\n",
            "2.38E-02\n",
            "2.27E-02\n",
            "2.31E-02\n",
            "2.46E-02\n",
            "2.03E-02\n",
            "2.82E-02\n",
            "2.56E-02\n",
            "2.19E-02\n",
            "2.40E-02\n",
            "2.38E-02\n",
            "2.27E-02\n",
            "2.48E-02\n",
            "2.06E-02\n",
            "2.33E-02\n",
            "2.04E-02\n",
            "1.94E-02\n",
            "1.99E-02\n",
            "2.17E-02\n",
            "2.31E-02\n",
            "2.08E-02\n",
            "1.90E-02\n",
            "2.04E-02\n",
            "1.85E-02\n",
            "1.96E-02\n",
            "2.14E-02\n",
            "1.66E-02\n",
            "1.70E-02\n",
            "2.06E-02\n",
            "1.88E-02\n",
            "1.88E-02\n",
            "1.99E-02\n",
            "1.75E-02\n",
            "2.06E-02\n",
            "1.70E-02\n",
            "1.79E-02\n",
            "1.68E-02\n",
            "1.76E-02\n",
            "1.77E-02\n",
            "1.72E-02\n",
            "1.60E-02\n",
            "1.27E-02\n",
            "1.48E-02\n",
            "1.61E-02\n",
            "1.43E-02\n",
            "1.51E-02\n",
            "1.61E-02\n",
            "1.44E-02\n",
            "1.55E-02\n",
            "1.29E-02\n",
            "1.51E-02\n",
            "1.28E-02\n",
            "1.30E-02\n",
            "1.34E-02\n",
            "1.16E-02\n",
            "1.35E-02\n",
            "1.28E-02\n",
            "1.27E-02\n",
            "1.23E-02\n",
            "1.30E-02\n",
            "1.17E-02\n",
            "1.26E-02\n",
            "1.19E-02\n",
            "1.20E-02\n",
            "1.27E-02\n",
            "1.13E-02\n",
            "1.52E-02\n",
            "1.33E-02\n",
            "1.29E-02\n",
            "1.14E-02\n",
            "1.27E-02\n",
            "1.13E-02\n",
            "1.09E-02\n",
            "1.23E-02\n",
            "1.29E-02\n",
            "1.18E-02\n",
            "1.14E-02\n",
            "1.11E-02\n",
            "1.07E-02\n",
            "1.12E-02\n",
            "1.09E-02\n",
            "9.43E-03\n",
            "7.86E-03\n",
            "7.90E-03\n",
            "1.04E-02\n",
            "1.26E-02\n",
            "7.87E-03\n",
            "8.56E-03\n",
            "1.03E-02\n",
            "1.13E-02\n",
            "8.29E-03\n",
            "8.41E-03\n",
            "8.77E-03\n",
            "8.91E-03\n",
            "9.68E-03\n",
            "1.04E-02\n",
            "9.53E-03\n",
            "1.03E-02\n",
            "9.95E-03\n",
            "9.27E-03\n",
            "8.95E-03\n",
            "7.73E-03\n",
            "1.02E-02\n",
            "8.33E-03\n",
            "9.61E-03\n",
            "8.46E-03\n",
            "7.47E-03\n",
            "8.80E-03\n",
            "1.04E-02\n",
            "7.86E-03\n",
            "9.47E-03\n",
            "9.59E-03\n",
            "9.43E-03\n",
            "8.36E-03\n",
            "8.63E-03\n",
            "8.21E-03\n",
            "7.67E-03\n",
            "6.75E-03\n",
            "8.03E-03\n",
            "9.41E-03\n",
            "8.22E-03\n",
            "7.24E-03\n",
            "7.91E-03\n",
            "7.15E-03\n",
            "7.71E-03\n",
            "8.63E-03\n",
            "7.98E-03\n",
            "7.43E-03\n",
            "6.68E-03\n",
            "7.35E-03\n",
            "8.39E-03\n",
            "8.03E-03\n",
            "6.48E-03\n",
            "6.89E-03\n",
            "8.22E-03\n",
            "6.78E-03\n",
            "7.55E-03\n",
            "6.51E-03\n",
            "7.71E-03\n",
            "7.85E-03\n",
            "5.65E-03\n",
            "6.42E-03\n",
            "8.17E-03\n",
            "7.06E-03\n",
            "6.13E-03\n",
            "7.36E-03\n",
            "7.68E-03\n",
            "7.48E-03\n",
            "6.96E-03\n",
            "6.99E-03\n",
            "7.92E-03\n",
            "7.84E-03\n",
            "5.34E-03\n",
            "7.43E-03\n",
            "6.74E-03\n",
            "7.63E-03\n",
            "8.71E-03\n",
            "9.29E-03\n",
            "5.34E-03\n",
            "6.34E-03\n",
            "5.93E-03\n",
            "5.06E-03\n",
            "6.81E-03\n",
            "6.96E-03\n",
            "6.41E-03\n",
            "6.87E-03\n",
            "8.58E-03\n",
            "6.99E-03\n",
            "6.37E-03\n",
            "6.58E-03\n",
            "5.22E-03\n",
            "6.18E-03\n",
            "6.61E-03\n",
            "6.50E-03\n",
            "5.65E-03\n",
            "6.72E-03\n",
            "5.25E-03\n",
            "6.28E-03\n",
            "6.56E-03\n",
            "4.94E-03\n",
            "5.56E-03\n",
            "6.06E-03\n",
            "6.05E-03\n",
            "7.38E-03\n",
            "4.61E-03\n",
            "5.12E-03\n",
            "6.75E-03\n",
            "6.33E-03\n",
            "6.35E-03\n",
            "4.70E-03\n",
            "5.39E-03\n",
            "6.26E-03\n",
            "6.37E-03\n",
            "5.51E-03\n",
            "5.64E-03\n",
            "6.49E-03\n",
            "4.88E-03\n",
            "6.04E-03\n",
            "5.99E-03\n",
            "6.28E-03\n",
            "5.63E-03\n",
            "4.07E-03\n",
            "5.05E-03\n",
            "5.76E-03\n",
            "5.90E-03\n",
            "6.41E-03\n",
            "4.56E-03\n",
            "6.38E-03\n",
            "5.66E-03\n",
            "6.21E-03\n",
            "4.46E-03\n",
            "4.59E-03\n",
            "5.59E-03\n",
            "4.59E-03\n",
            "5.55E-03\n",
            "6.81E-03\n",
            "5.37E-03\n",
            "5.69E-03\n",
            "5.09E-03\n",
            "6.20E-03\n",
            "4.07E-03\n",
            "5.88E-03\n",
            "4.73E-03\n",
            "5.84E-03\n",
            "6.24E-03\n",
            "6.21E-03\n",
            "4.28E-03\n",
            "3.72E-03\n",
            "4.66E-03\n",
            "5.60E-03\n",
            "5.97E-03\n",
            "6.01E-03\n",
            "5.13E-03\n",
            "5.66E-03\n",
            "5.39E-03\n",
            "4.56E-03\n",
            "4.42E-03\n",
            "2.93E-03\n",
            "3.48E-03\n",
            "5.31E-03\n",
            "4.50E-03\n",
            "4.77E-03\n",
            "4.75E-03\n",
            "4.19E-03\n",
            "5.40E-03\n",
            "5.24E-03\n",
            "3.41E-03\n",
            "5.09E-03\n",
            "4.55E-03\n",
            "4.40E-03\n",
            "5.46E-03\n",
            "4.64E-03\n",
            "4.51E-03\n",
            "5.02E-03\n",
            "4.75E-03\n",
            "4.60E-03\n",
            "4.33E-03\n",
            "4.47E-03\n",
            "5.21E-03\n",
            "4.31E-03\n",
            "5.56E-03\n",
            "3.78E-03\n",
            "3.88E-03\n",
            "3.24E-03\n",
            "5.51E-03\n",
            "5.50E-03\n",
            "4.61E-03\n",
            "4.96E-03\n",
            "5.34E-03\n",
            "4.06E-03\n",
            "5.94E-03\n",
            "4.05E-03\n",
            "4.29E-03\n",
            "5.65E-03\n",
            "5.27E-03\n",
            "4.63E-03\n",
            "3.63E-03\n",
            "4.61E-03\n",
            "4.23E-03\n",
            "3.98E-03\n",
            "3.96E-03\n",
            "4.20E-03\n",
            "4.80E-03\n",
            "3.69E-03\n",
            "3.93E-03\n",
            "4.27E-03\n",
            "4.16E-03\n",
            "5.46E-03\n",
            "4.37E-03\n",
            "6.64E-03\n",
            "5.31E-03\n",
            "4.82E-03\n",
            "5.16E-03\n",
            "3.96E-03\n",
            "3.35E-03\n",
            "4.05E-03\n",
            "5.35E-03\n",
            "5.69E-03\n",
            "4.26E-03\n",
            "4.48E-03\n",
            "2.95E-03\n",
            "4.58E-03\n",
            "6.32E-03\n",
            "3.39E-03\n",
            "4.66E-03\n",
            "4.42E-03\n",
            "3.71E-03\n",
            "4.05E-03\n",
            "3.70E-03\n",
            "3.80E-03\n",
            "4.02E-03\n",
            "4.47E-03\n",
            "4.34E-03\n",
            "4.90E-03\n",
            "3.65E-03\n",
            "3.86E-03\n",
            "4.64E-03\n",
            "4.40E-03\n",
            "3.72E-03\n",
            "3.94E-03\n",
            "4.71E-03\n",
            "3.36E-03\n",
            "2.79E-03\n",
            "4.68E-03\n",
            "5.44E-03\n",
            "4.54E-03\n",
            "5.41E-03\n",
            "3.09E-03\n",
            "4.62E-03\n",
            "3.95E-03\n",
            "4.16E-03\n",
            "2.30E-03\n",
            "4.68E-03\n",
            "4.02E-03\n",
            "3.58E-03\n",
            "4.22E-03\n",
            "4.42E-03\n",
            "3.66E-03\n",
            "3.22E-03\n",
            "3.75E-03\n",
            "4.38E-03\n",
            "3.62E-03\n",
            "4.36E-03\n",
            "2.86E-03\n",
            "2.54E-03\n",
            "5.38E-03\n",
            "3.58E-03\n",
            "4.20E-03\n",
            "4.30E-03\n",
            "2.61E-03\n",
            "3.96E-03\n",
            "2.60E-03\n",
            "3.95E-03\n",
            "3.21E-03\n",
            "3.72E-03\n",
            "3.81E-03\n",
            "3.19E-03\n",
            "4.10E-03\n",
            "3.68E-03\n",
            "2.96E-03\n",
            "3.87E-03\n",
            "4.27E-03\n",
            "3.65E-03\n",
            "4.25E-03\n",
            "4.24E-03\n",
            "3.12E-03\n",
            "3.51E-03\n",
            "4.11E-03\n",
            "3.90E-03\n",
            "1.99E-03\n",
            "3.18E-03\n",
            "4.17E-03\n",
            "3.37E-03\n",
            "3.46E-03\n",
            "3.35E-03\n",
            "3.34E-03\n",
            "3.04E-03\n",
            "3.33E-03\n",
            "3.81E-03\n",
            "3.70E-03\n",
            "3.40E-03\n",
            "2.81E-03\n",
            "3.29E-03\n",
            "3.76E-03\n",
            "4.14E-03\n",
            "3.36E-03\n",
            "3.83E-03\n",
            "3.82E-03\n",
            "2.95E-03\n",
            "2.57E-03\n",
            "2.65E-03\n",
            "2.84E-03\n",
            "2.93E-03\n",
            "1.79E-03\n",
            "4.04E-03\n",
            "2.72E-03\n",
            "4.40E-03\n",
            "3.45E-03\n",
            "3.44E-03\n",
            "2.97E-03\n",
            "3.33E-03\n",
            "2.50E-03\n",
            "3.50E-03\n",
            "2.67E-03\n",
            "3.12E-03\n",
            "3.20E-03\n",
            "3.11E-03\n",
            "3.74E-03\n",
            "3.18E-03\n",
            "3.17E-03\n",
            "3.98E-03\n",
            "3.97E-03\n",
            "3.42E-03\n",
            "3.59E-03\n",
            "2.06E-03\n",
            "3.31E-03\n",
            "4.10E-03\n",
            "2.40E-03\n",
            "3.64E-03\n",
            "2.74E-03\n",
            "3.53E-03\n",
            "3.52E-03\n",
            "3.34E-03\n",
            "2.63E-03\n",
            "2.80E-03\n",
            "2.61E-03\n",
            "2.61E-03\n",
            "3.64E-03\n",
            "2.85E-03\n",
            "3.28E-03\n",
            "3.18E-03\n",
            "2.92E-03\n",
            "2.40E-03\n",
            "3.08E-03\n",
            "3.49E-03\n",
            "3.15E-03\n",
            "2.12E-03\n",
            "2.96E-03\n",
            "3.71E-03\n",
            "3.62E-03\n",
            "3.11E-03\n",
            "3.01E-03\n",
            "3.09E-03\n",
            "2.42E-03\n",
            "2.91E-03\n",
            "3.15E-03\n",
            "2.89E-03\n",
            "3.38E-03\n",
            "3.29E-03\n",
            "2.63E-03\n",
            "2.70E-03\n",
            "2.86E-03\n",
            "3.26E-03\n",
            "3.09E-03\n",
            "2.43E-03\n",
            "1.78E-03\n",
            "2.50E-03\n",
            "3.62E-03\n",
            "3.77E-03\n",
            "3.12E-03\n",
            "3.19E-03\n",
            "2.39E-03\n",
            "3.33E-03\n",
            "3.24E-03\n",
            "2.84E-03\n",
            "3.38E-03\n",
            "2.12E-03\n",
            "2.19E-03\n",
            "2.58E-03\n",
            "2.34E-03\n",
            "2.41E-03\n",
            "2.71E-03\n",
            "3.48E-03\n",
            "2.31E-03\n",
            "2.00E-03\n",
            "3.07E-03\n",
            "2.30E-03\n",
            "2.75E-03\n",
            "3.20E-03\n",
            "2.73E-03\n",
            "2.73E-03\n",
            "2.42E-03\n",
            "1.88E-03\n",
            "2.10E-03\n",
            "2.32E-03\n",
            "2.69E-03\n",
            "2.98E-03\n",
            "2.38E-03\n",
            "2.97E-03\n",
            "2.37E-03\n",
            "2.14E-03\n",
            "2.87E-03\n",
            "3.31E-03\n",
            "2.13E-03\n",
            "2.49E-03\n",
            "1.97E-03\n",
            "2.55E-03\n",
            "2.61E-03\n",
            "3.55E-03\n",
            "2.82E-03\n",
            "2.16E-03\n",
            "2.37E-03\n",
            "3.22E-03\n",
            "2.50E-03\n",
            "2.35E-03\n",
            "2.70E-03\n",
            "2.76E-03\n",
            "1.91E-03\n",
            "1.76E-03\n",
            "2.32E-03\n",
            "2.52E-03\n",
            "2.52E-03\n",
            "2.65E-03\n",
            "1.67E-03\n",
            "2.50E-03\n",
            "2.08E-03\n",
            "2.28E-03\n",
            "2.89E-03\n",
            "2.34E-03\n",
            "3.01E-03\n",
            "2.05E-03\n",
            "2.52E-03\n",
            "2.45E-03\n",
            "2.03E-03\n",
            "2.64E-03\n",
            "2.43E-03\n",
            "2.49E-03\n",
            "1.88E-03\n",
            "2.14E-03\n",
            "1.40E-03\n",
            "2.73E-03\n",
            "2.46E-03\n",
            "2.18E-03\n",
            "2.24E-03\n",
            "2.24E-03\n",
            "2.10E-03\n",
            "2.62E-03\n",
            "2.42E-03\n",
            "2.15E-03\n",
            "2.86E-03\n",
            "2.66E-03\n",
            "2.52E-03\n",
            "2.26E-03\n",
            "2.06E-03\n",
            "1.99E-03\n",
            "2.75E-03\n",
            "2.74E-03\n",
            "2.99E-03\n",
            "2.41E-03\n",
            "2.40E-03\n",
            "2.46E-03\n",
            "2.45E-03\n",
            "2.00E-03\n",
            "2.37E-03\n",
            "2.18E-03\n",
            "1.80E-03\n",
            "2.35E-03\n",
            "1.11E-03\n",
            "2.22E-03\n",
            "2.40E-03\n",
            "2.33E-03\n",
            "2.32E-03\n",
            "2.56E-03\n",
            "2.25E-03\n",
            "2.42E-03\n",
            "1.81E-03\n",
            "1.87E-03\n",
            "1.98E-03\n",
            "2.99E-03\n",
            "1.67E-03\n",
            "2.20E-03\n",
            "1.84E-03\n",
            "2.49E-03\n",
            "2.48E-03\n",
            "1.88E-03\n",
            "1.70E-03\n",
            "1.87E-03\n",
            "2.22E-03\n",
            "1.75E-03\n",
            "2.44E-03\n",
            "2.32E-03\n",
            "2.19E-03\n",
            "1.61E-03\n",
            "2.35E-03\n",
            "2.00E-03\n",
            "1.83E-03\n",
            "2.16E-03\n",
            "1.70E-03\n",
            "1.58E-03\n",
            "1.81E-03\n",
            "1.74E-03\n",
            "2.47E-03\n",
            "2.41E-03\n",
            "1.95E-03\n",
            "2.39E-03\n",
            "2.77E-03\n",
            "2.10E-03\n",
            "1.87E-03\n",
            "2.36E-03\n",
            "1.70E-03\n",
            "2.08E-03\n",
            "1.80E-03\n",
            "2.01E-03\n",
            "1.68E-03\n",
            "2.75E-03\n",
            "1.94E-03\n",
            "1.72E-03\n",
            "2.09E-03\n",
            "1.76E-03\n",
            "1.91E-03\n",
            "2.17E-03\n",
            "1.96E-03\n",
            "2.32E-03\n",
            "1.79E-03\n",
            "1.57E-03\n",
            "1.78E-03\n",
            "1.87E-03\n",
            "1.35E-03\n",
            "1.71E-03\n",
            "1.81E-03\n",
            "1.39E-03\n",
            "1.85E-03\n",
            "1.79E-03\n",
            "1.48E-03\n",
            "1.68E-03\n",
            "1.67E-03\n",
            "1.72E-03\n",
            "1.92E-03\n",
            "1.76E-03\n",
            "1.81E-03\n",
            "2.05E-03\n",
            "1.75E-03\n",
            "1.79E-03\n",
            "1.69E-03\n",
            "2.08E-03\n",
            "1.92E-03\n",
            "1.13E-03\n",
            "1.81E-03\n",
            "1.61E-03\n",
            "1.56E-03\n",
            "1.65E-03\n",
            "1.36E-03\n",
            "1.21E-03\n",
            "1.54E-03\n",
            "1.49E-03\n",
            "1.58E-03\n",
            "1.77E-03\n",
            "1.57E-03\n",
            "1.85E-03\n",
            "1.66E-03\n",
            "1.75E-03\n",
            "1.84E-03\n",
            "2.21E-03\n",
            "2.16E-03\n",
            "2.15E-03\n",
            "1.82E-03\n",
            "1.53E-03\n",
            "1.43E-03\n",
            "1.57E-03\n",
            "1.66E-03\n",
            "1.74E-03\n",
            "1.42E-03\n",
            "1.46E-03\n",
            "1.50E-03\n",
            "2.08E-03\n",
            "1.49E-03\n",
            "1.49E-03\n",
            "1.71E-03\n",
            "1.39E-03\n",
            "1.92E-03\n",
            "1.47E-03\n",
            "1.77E-03\n",
            "1.77E-03\n",
            "2.11E-03\n",
            "1.23E-03\n",
            "1.44E-03\n",
            "1.75E-03\n",
            "1.31E-03\n",
            "1.47E-03\n",
            "1.34E-03\n",
            "1.47E-03\n",
            "1.85E-03\n",
            "1.76E-03\n",
            "1.49E-03\n",
            "1.36E-03\n",
            "1.66E-03\n",
            "1.65E-03\n",
            "1.18E-03\n",
            "1.30E-03\n",
            "1.68E-03\n",
            "1.42E-03\n",
            "1.50E-03\n",
            "1.29E-03\n",
            "1.57E-03\n",
            "1.16E-03\n",
            "1.23E-03\n",
            "1.72E-03\n",
            "1.31E-03\n",
            "1.75E-03\n",
            "1.63E-03\n",
            "1.22E-03\n",
            "1.29E-03\n",
            "1.65E-03\n",
            "1.37E-03\n",
            "1.64E-03\n",
            "1.48E-03\n",
            "1.39E-03\n",
            "1.15E-03\n",
            "1.30E-03\n",
            "1.26E-03\n",
            "1.49E-03\n",
            "1.29E-03\n",
            "1.17E-03\n",
            "1.05E-03\n",
            "1.20E-03\n",
            "1.39E-03\n",
            "1.81E-03\n",
            "1.00E-03\n",
            "1.61E-03\n",
            "1.42E-03\n",
            "1.79E-03\n",
            "1.41E-03\n",
            "1.17E-03\n",
            "1.59E-03\n",
            "1.51E-03\n",
            "1.20E-03\n",
            "1.35E-03\n",
            "1.01E-03\n",
            "1.23E-03\n",
            "1.04E-03\n",
            "1.26E-03\n",
            "1.58E-03\n",
            "1.25E-03\n",
            "1.46E-03\n",
            "1.31E-03\n",
            "1.38E-03\n",
            "1.20E-03\n",
            "1.08E-03\n",
            "1.19E-03\n",
            "1.40E-03\n",
            "1.25E-03\n",
            "1.18E-03\n",
            "8.54E-04\n",
            "1.31E-03\n",
            "1.34E-03\n",
            "1.52E-03\n",
            "1.09E-03\n",
            "1.40E-03\n",
            "1.36E-03\n",
            "1.46E-03\n",
            "1.21E-03\n",
            "1.69E-03\n",
            "8.96E-04\n",
            "1.58E-03\n",
            "1.20E-03\n",
            "1.30E-03\n",
            "1.16E-03\n",
            "1.29E-03\n",
            "1.18E-03\n",
            "9.09E-04\n",
            "1.11E-03\n",
            "1.20E-03\n",
            "1.37E-03\n",
            "8.97E-04\n",
            "1.39E-03\n",
            "1.16E-03\n",
            "1.51E-03\n",
            "1.11E-03\n",
            "1.37E-03\n",
            "9.77E-04\n",
            "1.04E-03\n",
            "1.00E-03\n",
            "1.35E-03\n",
            "1.12E-03\n",
            "1.22E-03\n",
            "1.09E-03\n",
            "9.55E-04\n",
            "9.20E-04\n",
            "1.07E-03\n",
            "1.10E-03\n",
            "6.28E-04\n",
            "1.41E-03\n",
            "1.28E-03\n",
            "9.33E-04\n",
            "1.43E-03\n",
            "1.17E-03\n",
            "1.11E-03\n",
            "1.07E-03\n",
            "1.41E-03\n",
            "1.07E-03\n",
            "1.25E-03\n",
            "1.21E-03\n",
            "1.18E-03\n",
            "1.29E-03\n",
            "1.17E-03\n",
            "8.66E-04\n",
            "1.19E-03\n",
            "8.31E-04\n",
            "8.87E-04\n",
            "1.03E-03\n",
            "9.69E-04\n",
            "1.52E-03\n",
            "1.25E-03\n",
            "1.08E-03\n",
            "1.04E-03\n",
            "1.18E-03\n",
            "1.12E-03\n",
            "9.74E-04\n",
            "9.14E-04\n",
            "8.82E-04\n",
            "1.02E-03\n",
            "8.76E-04\n",
            "9.86E-04\n",
            "1.01E-03\n",
            "9.23E-04\n",
            "1.14E-03\n",
            "9.45E-04\n",
            "9.97E-04\n",
            "8.00E-04\n",
            "1.13E-03\n",
            "9.32E-04\n",
            "1.12E-03\n",
            "1.23E-03\n",
            "1.03E-03\n",
            "1.24E-03\n",
            "7.54E-04\n",
            "9.13E-04\n",
            "9.90E-04\n",
            "1.01E-03\n",
            "8.76E-04\n",
            "8.21E-04\n",
            "1.00E-03\n",
            "8.94E-04\n",
            "9.43E-04\n",
            "8.88E-04\n",
            "9.89E-04\n",
            "6.48E-04\n",
            "1.09E-03\n",
            "8.50E-04\n",
            "8.98E-04\n",
            "1.02E-03\n",
            "9.94E-04\n",
            "9.65E-04\n",
            "7.34E-04\n",
            "9.59E-04\n",
            "8.30E-04\n",
            "1.05E-03\n",
            "8.99E-04\n",
            "8.21E-04\n",
            "1.07E-03\n",
            "1.06E-03\n",
            "8.37E-04\n",
            "8.09E-04\n",
            "9.29E-04\n",
            "7.79E-04\n",
            "7.04E-04\n",
            "8.46E-04\n",
            "8.19E-04\n",
            "1.08E-03\n",
            "8.86E-04\n",
            "9.54E-04\n",
            "9.03E-04\n",
            "7.58E-04\n",
            "8.49E-04\n",
            "9.64E-04\n",
            "1.03E-03\n",
            "9.10E-04\n",
            "9.07E-04\n",
            "9.73E-04\n",
            "7.62E-04\n",
            "9.43E-04\n",
            "8.48E-04\n",
            "6.39E-04\n",
            "9.32E-04\n",
            "9.52E-04\n",
            "7.45E-04\n",
            "7.65E-04\n",
            "1.12E-03\n",
            "7.59E-04\n",
            "9.12E-04\n",
            "7.31E-04\n",
            "1.02E-03\n",
            "6.38E-04\n",
            "7.67E-04\n",
            "7.20E-04\n",
            "8.92E-04\n",
            "6.50E-04\n",
            "8.20E-04\n",
            "5.59E-04\n",
            "8.57E-04\n",
            "6.41E-04\n",
            "4.26E-04\n",
            "9.76E-04\n",
            "7.40E-04\n",
            "7.79E-04\n",
            "8.60E-04\n",
            "7.32E-04\n",
            "7.71E-04\n",
            "7.06E-04\n",
            "8.48E-04\n",
            "7.21E-04\n",
            "7.39E-04\n",
            "9.00E-04\n",
            "7.54E-04\n",
            "8.33E-04\n",
            "6.88E-04\n",
            "7.06E-04\n",
            "7.83E-04\n",
            "7.00E-04\n",
            "6.98E-04\n",
            "8.74E-04\n",
            "8.31E-04\n",
            "8.48E-04\n",
            "6.28E-04\n",
            "9.98E-04\n",
            "8.57E-04\n",
            "8.93E-04\n",
            "8.89E-04\n",
            "6.16E-04\n",
            "6.13E-04\n",
            "7.26E-04\n",
            "6.85E-04\n",
            "4.36E-04\n",
            "5.10E-04\n",
            "7.71E-04\n",
            "5.81E-04\n",
            "7.28E-04\n",
            "6.69E-04\n",
            "5.37E-04\n",
            "5.91E-04\n",
            "7.17E-04\n",
            "6.60E-04\n",
            "7.67E-04\n",
            "6.00E-04\n",
            "5.80E-04\n",
            "8.49E-04\n",
            "6.48E-04\n",
            "4.84E-04\n",
            "6.25E-04\n",
            "6.94E-04\n",
            "7.44E-04\n",
            "5.65E-04\n",
            "6.51E-04\n",
            "5.96E-04\n",
            "8.21E-04\n",
            "8.00E-04\n",
            "5.54E-04\n",
            "6.39E-04\n",
            "6.19E-04\n",
            "5.14E-04\n",
            "6.82E-04\n",
            "5.95E-04\n",
            "6.26E-04\n",
            "6.75E-04\n",
            "5.21E-04\n",
            "5.86E-04\n",
            "5.34E-04\n",
            "6.31E-04\n",
            "8.77E-04\n",
            "3.79E-04\n",
            "4.93E-04\n",
            "6.38E-04\n",
            "5.87E-04\n",
            "5.20E-04\n",
            "7.28E-04\n",
            "6.61E-04\n",
            "3.69E-04\n",
            "5.60E-04\n",
            "5.90E-04\n",
            "6.19E-04\n",
            "5.54E-04\n",
            "4.88E-04\n",
            "5.96E-04\n",
            "5.63E-04\n",
            "4.36E-04\n",
            "5.28E-04\n",
            "6.03E-04\n",
            "7.70E-04\n",
            "4.76E-04\n",
            "5.66E-04\n",
            "5.64E-04\n",
            "4.25E-04\n",
            "4.53E-04\n",
            "6.93E-04\n",
            "6.00E-04\n",
            "5.38E-04\n",
            "6.40E-04\n",
            "5.78E-04\n",
            "5.17E-04\n",
            "5.74E-04\n",
            "5.57E-04\n",
            "5.25E-04\n",
            "6.11E-04\n",
            "5.07E-04\n",
            "6.34E-04\n",
            "4.74E-04\n",
            "4.01E-04\n",
            "4.42E-04\n",
            "5.68E-04\n",
            "5.09E-04\n",
            "4.51E-04\n",
            "4.63E-04\n",
            "5.03E-04\n",
            "4.18E-04\n",
            "4.30E-04\n",
            "6.08E-04\n",
            "4.27E-04\n",
            "4.66E-04\n",
            "4.23E-04\n",
            "3.54E-04\n",
            "6.78E-04\n",
            "3.38E-04\n",
            "5.25E-04\n",
            "4.56E-04\n",
            "5.34E-04\n",
            "4.52E-04\n",
            "3.98E-04\n",
            "4.09E-04\n",
            "3.42E-04\n",
            "3.28E-04\n",
            "5.09E-04\n",
            "4.82E-04\n",
            "3.89E-04\n",
            "4.26E-04\n",
            "4.25E-04\n",
            "5.00E-04\n",
            "4.60E-04\n",
            "5.73E-04\n",
            "4.95E-04\n",
            "4.30E-04\n",
            "3.78E-04\n",
            "5.02E-04\n",
            "3.87E-04\n",
            "4.23E-04\n",
            "2.60E-04\n",
            "3.09E-04\n",
            "4.43E-04\n",
            "3.92E-04\n",
            "5.62E-04\n",
            "4.14E-04\n",
            "4.73E-04\n",
            "4.11E-04\n",
            "4.93E-04\n",
            "4.67E-04\n",
            "4.66E-04\n",
            "3.92E-04\n",
            "4.62E-04\n",
            "4.25E-04\n",
            "4.35E-04\n",
            "3.98E-04\n",
            "3.61E-04\n",
            "2.67E-04\n",
            "5.44E-04\n",
            "3.34E-04\n",
            "3.21E-04\n",
            "4.35E-04\n",
            "5.24E-04\n",
            "3.63E-04\n",
            "2.94E-04\n",
            "4.73E-04\n",
            "2.92E-04\n",
            "3.80E-04\n",
            "3.78E-04\n",
            "4.32E-04\n",
            "3.64E-04\n",
            "4.18E-04\n",
            "2.96E-04\n",
            "4.48E-04\n",
            "4.46E-04\n",
            "3.36E-04\n",
            "3.88E-04\n",
            "4.84E-04\n",
            "4.28E-04\n",
            "2.56E-04\n",
            "3.93E-04\n",
            "3.91E-04\n",
            "4.11E-04\n",
            "2.94E-04\n",
            "3.66E-04\n",
            "3.54E-04\n",
            "2.90E-04\n",
            "3.72E-04\n",
            "3.09E-04\n",
            "3.59E-04\n",
            "4.08E-04\n",
            "3.76E-04\n",
            "3.24E-04\n",
            "4.24E-04\n",
            "3.21E-04\n",
            "3.40E-04\n",
            "3.19E-04\n",
            "3.47E-04\n",
            "3.56E-04\n",
            "4.04E-04\n",
            "3.04E-04\n",
            "3.13E-04\n",
            "4.28E-04\n",
            "3.49E-04\n",
            "3.86E-04\n",
            "4.04E-04\n",
            "2.78E-04\n",
            "3.62E-04\n",
            "4.27E-04\n",
            "2.46E-04\n",
            "2.92E-04\n",
            "2.81E-04\n",
            "3.18E-04\n",
            "2.70E-04\n",
            "3.24E-04\n",
            "3.69E-04\n",
            "3.31E-04\n",
            "3.48E-04\n",
            "3.83E-04\n",
            "3.36E-04\n",
            "3.53E-04\n",
            "3.60E-04\n",
            "3.41E-04\n",
            "2.59E-04\n",
            "3.29E-04\n",
            "2.92E-04\n",
            "3.00E-04\n",
            "3.25E-04\n",
            "2.27E-04\n",
            "3.31E-04\n",
            "2.60E-04\n",
            "3.71E-04\n",
            "2.49E-04\n",
            "3.08E-04\n",
            "2.73E-04\n",
            "3.06E-04\n",
            "2.96E-04\n",
            "3.46E-04\n",
            "2.94E-04\n",
            "2.09E-04\n",
            "3.16E-04\n",
            "2.40E-04\n",
            "2.48E-04\n",
            "3.54E-04\n",
            "3.36E-04\n",
            "1.96E-04\n",
            "3.25E-04\n",
            "2.67E-04\n",
            "2.26E-04\n",
            "3.13E-04\n",
            "3.28E-04\n",
            "3.11E-04\n",
            "2.62E-04\n",
            "3.24E-04\n",
            "2.12E-04\n",
            "3.68E-04\n",
            "2.89E-04\n",
            "2.25E-04\n",
            "2.86E-04\n",
            "2.93E-04\n",
            "2.99E-04\n",
            "2.82E-04\n",
            "2.89E-04\n",
            "2.65E-04\n",
            "1.88E-04\n",
            "2.70E-04\n",
            "2.76E-04\n",
            "2.60E-04\n",
            "2.30E-04\n",
            "2.66E-04\n",
            "3.16E-04\n",
            "2.93E-04\n",
            "3.57E-04\n",
            "2.47E-04\n",
            "2.38E-04\n",
            "3.02E-04\n",
            "3.01E-04\n",
            "2.64E-04\n",
            "2.77E-04\n",
            "3.11E-04\n",
            "3.30E-04\n",
            "2.38E-04\n",
            "3.06E-04\n",
            "3.05E-04\n",
            "2.28E-04\n",
            "2.33E-04\n",
            "1.91E-04\n",
            "3.06E-04\n",
            "2.44E-04\n",
            "2.83E-04\n",
            "2.35E-04\n",
            "2.07E-04\n",
            "1.86E-04\n",
            "2.25E-04\n",
            "2.04E-04\n",
            "2.30E-04\n",
            "2.29E-04\n",
            "2.54E-04\n",
            "2.52E-04\n",
            "2.00E-04\n",
            "2.25E-04\n",
            "2.11E-04\n",
            "1.91E-04\n",
            "2.41E-04\n",
            "2.59E-04\n",
            "1.63E-04\n",
            "1.75E-04\n",
            "2.30E-04\n",
            "2.60E-04\n",
            "2.66E-04\n",
            "2.34E-04\n",
            "2.26E-04\n",
            "1.71E-04\n",
            "2.55E-04\n",
            "1.87E-04\n",
            "2.17E-04\n",
            "2.10E-04\n",
            "1.73E-04\n",
            "1.90E-04\n",
            "2.25E-04\n",
            "2.00E-04\n",
            "2.29E-04\n",
            "1.98E-04\n",
            "2.15E-04\n",
            "2.02E-04\n",
            "2.13E-04\n",
            "2.24E-04\n",
            "2.00E-04\n",
            "2.56E-04\n",
            "2.04E-04\n",
            "2.03E-04\n",
            "2.36E-04\n",
            "2.01E-04\n",
            "1.50E-04\n",
            "1.83E-04\n",
            "1.71E-04\n",
            "1.86E-04\n",
            "1.97E-04\n",
            "1.41E-04\n",
            "1.57E-04\n",
            "2.37E-04\n",
            "2.63E-04\n",
            "1.23E-04\n",
            "2.23E-04\n",
            "2.01E-04\n",
            "1.42E-04\n",
            "1.68E-04\n",
            "1.72E-04\n",
            "1.98E-04\n",
            "2.23E-04\n",
            "1.60E-04\n",
            "2.57E-04\n",
            "1.84E-04\n",
            "1.73E-04\n",
            "1.82E-04\n",
            "1.11E-04\n",
            "1.76E-04\n",
            "1.90E-04\n",
            "1.74E-04\n",
            "1.68E-04\n",
            "1.77E-04\n",
            "1.82E-04\n",
            "1.51E-04\n",
            "1.80E-04\n",
            "1.55E-04\n",
            "1.64E-04\n",
            "1.87E-04\n",
            "1.77E-04\n",
            "1.33E-04\n",
            "1.80E-04\n",
            "1.60E-04\n",
            "2.07E-04\n",
            "1.40E-04\n",
            "1.86E-04\n",
            "1.62E-04\n",
            "1.66E-04\n",
            "1.47E-04\n",
            "1.87E-04\n",
            "2.14E-04\n",
            "1.77E-04\n",
            "1.49E-04\n",
            "1.44E-04\n",
            "1.61E-04\n",
            "1.64E-04\n",
            "1.81E-04\n",
            "1.85E-04\n",
            "1.62E-04\n",
            "1.22E-04\n",
            "1.56E-04\n",
            "1.55E-04\n",
            "1.85E-04\n",
            "1.24E-04\n",
            "1.79E-04\n",
            "1.02E-04\n",
            "1.52E-04\n",
            "1.55E-04\n",
            "1.59E-04\n",
            "1.50E-04\n",
            "1.82E-04\n",
            "1.69E-04\n",
            "1.56E-04\n",
            "1.63E-04\n",
            "1.42E-04\n",
            "1.25E-04\n",
            "1.73E-04\n",
            "1.44E-04\n",
            "1.63E-04\n",
            "1.75E-04\n",
            "1.38E-04\n",
            "9.03E-05\n",
            "1.29E-04\n",
            "1.59E-04\n",
            "1.55E-04\n",
            "1.31E-04\n",
            "1.30E-04\n",
            "1.45E-04\n",
            "1.25E-04\n",
            "1.13E-04\n",
            "1.58E-04\n",
            "1.65E-04\n",
            "1.19E-04\n",
            "1.59E-04\n",
            "1.29E-04\n",
            "1.10E-04\n",
            "1.21E-04\n",
            "1.53E-04\n",
            "1.19E-04\n",
            "1.41E-04\n",
            "1.29E-04\n",
            "1.07E-04\n",
            "1.56E-04\n",
            "1.24E-04\n",
            "1.13E-04\n",
            "1.51E-04\n",
            "1.22E-04\n",
            "1.39E-04\n",
            "1.07E-04\n",
            "1.17E-04\n",
            "1.09E-04\n",
            "1.26E-04\n",
            "1.12E-04\n",
            "1.28E-04\n",
            "8.38E-05\n",
            "1.17E-04\n",
            "1.10E-04\n",
            "1.16E-04\n",
            "1.15E-04\n",
            "1.08E-04\n",
            "1.44E-04\n",
            "1.07E-04\n",
            "1.16E-04\n",
            "1.51E-04\n",
            "9.92E-05\n",
            "1.05E-04\n",
            "9.19E-05\n",
            "1.33E-04\n",
            "1.04E-04\n",
            "1.34E-04\n",
            "9.95E-05\n",
            "1.08E-04\n",
            "9.86E-05\n",
            "1.13E-04\n",
            "6.71E-05\n",
            "1.28E-04\n",
            "1.12E-04\n",
            "1.05E-04\n",
            "9.88E-05\n",
            "9.24E-05\n",
            "1.19E-04\n",
            "1.00E-04\n",
            "9.11E-05\n",
            "1.20E-04\n",
            "9.90E-05\n",
            "1.04E-04\n",
            "1.36E-04\n",
            "1.21E-04\n",
            "1.23E-04\n",
            "9.09E-05\n",
            "1.07E-04\n",
            "1.01E-04\n",
            "9.23E-05\n",
            "8.91E-05\n",
            "1.05E-04\n",
            "1.02E-04\n",
            "1.32E-04\n",
            "9.55E-05\n",
            "9.50E-05\n",
            "9.19E-05\n",
            "9.14E-05\n",
            "8.02E-05\n",
            "8.78E-05\n",
            "9.01E-05\n",
            "1.00E-04\n",
            "7.87E-05\n",
            "8.35E-05\n",
            "1.09E-04\n",
            "8.79E-05\n",
            "1.11E-04\n",
            "8.96E-05\n",
            "7.64E-05\n",
            "6.84E-05\n",
            "1.19E-04\n",
            "7.28E-05\n",
            "8.49E-05\n",
            "9.45E-05\n",
            "8.41E-05\n",
            "8.86E-05\n",
            "5.64E-05\n",
            "8.54E-05\n",
            "7.28E-05\n",
            "7.01E-05\n",
            "9.62E-05\n",
            "8.62E-05\n",
            "8.10E-05\n",
            "8.06E-05\n",
            "9.67E-05\n",
            "4.93E-05\n",
            "7.01E-05\n",
            "6.51E-05\n",
            "8.57E-05\n",
            "7.15E-05\n",
            "6.43E-05\n",
            "9.14E-05\n",
            "7.28E-05\n",
            "9.96E-05\n",
            "7.66E-05\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b669f8f6f3ab>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# We use our model to predict the y values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# We compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b669f8f6f3ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;34m\"\"\"Forward.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"Example of a simple neural network with PyTorch.\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    \"\"\"Network class.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Init.\"\"\"\n",
        "        super(Net, self).__init__()\n",
        "        self.A = torch.nn.Linear(1, 32)\n",
        "        self.B = torch.nn.Linear(32, 32)\n",
        "        self.C = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward.\"\"\"\n",
        "        x = torch.relu(self.A(x))\n",
        "        x = torch.relu(self.B(x))\n",
        "        x = self.C(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Create model\n",
        "model = Net()\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "while True:\n",
        "\n",
        "    # We create some random integergers. Careful here, the requires_grad is\n",
        "    # very important. If we do all the computations without gradients it\n",
        "    # will naturally not work\n",
        "    x = torch.randint(\n",
        "        0, 7, (250, 1), dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    # We create the real y values. The objectif for the model is to give\n",
        "    # the squared value of x\n",
        "    y_true = torch.square(x)\n",
        "\n",
        "    # We use our model to predict the y values\n",
        "    y = model(x)\n",
        "\n",
        "    # We compute the loss\n",
        "    loss = torch.mean(torch.square(y - y_true))\n",
        "\n",
        "    # Print loss: it should decreases!\n",
        "    print(\"{:.2E}\".format(loss.item()))\n",
        "\n",
        "    # Reset the gradients to 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute the gradients of the model parameters relative to the loss\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the network\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ee1a88-f2be-4c13-8fed-07d5c429345c",
      "metadata": {
        "id": "f3ee1a88-f2be-4c13-8fed-07d5c429345c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\"\"\"Actor network (default).\"\"\"\n",
        "\n",
        "class ActorModelV0(nn.Module):\n",
        "    \"\"\"Deep neural network.\"\"\"\n",
        "\n",
        "    # By default, use CPU\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize model.\"\"\"\n",
        "        super(ActorModelV0, self).__init__()\n",
        "        # ---> TODO: change input and output sizes depending on the environment\n",
        "        input_size = 4\n",
        "        nb_actions = 2\n",
        "\n",
        "        # Build layer objects\n",
        "        self.fc0 = nn.Linear(input_size, 128)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.policy = nn.Linear(128, nb_actions)\n",
        "\n",
        "    def _preprocessor(self, state):\n",
        "        \"\"\"Preprocessor function.\n",
        "\n",
        "        Args:\n",
        "            state (numpy.array): environment state.\n",
        "\n",
        "        Returns:\n",
        "            x (torch.tensor): preprocessed state.\n",
        "        \"\"\"\n",
        "        # Add batch dimension\n",
        "        x = np.expand_dims(state, 0)\n",
        "\n",
        "        # Transform to torch.tensor\n",
        "        x = torch.from_numpy(x).float().to(self.DEVICE)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (numpy.array): environment state.\n",
        "\n",
        "        Returns:\n",
        "            actions_prob (torch.tensor): list with the probability of each\n",
        "                action over the action space.\n",
        "        \"\"\"\n",
        "        # Preprocessor\n",
        "        x = self._preprocessor(x)\n",
        "\n",
        "        # Input layer\n",
        "        x = F.relu(self.fc0(x))\n",
        "\n",
        "        # Middle layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Policy\n",
        "        action_prob = F.softmax(self.policy(x), dim=-1)\n",
        "\n",
        "        return action_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73f935c-9081-47f3-aa24-46d33c8dd4e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f73f935c-9081-47f3-aa24-46d33c8dd4e9",
        "outputId": "e079ae0f-7de5-4b34-f693-6ca7a5190695",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorModelV0(\n",
            "  (fc0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (policy): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n",
            "total_reward = 9.0\n"
          ]
        }
      ],
      "source": [
        "# Script to run a deep neural network policy\n",
        "# ------------------------------------------\n",
        "\n",
        "# Create environment and policy\n",
        "env = CartpoleEnvV0()\n",
        "policy = ActorModelV0()\n",
        "\n",
        "# Testing mode\n",
        "policy.eval()\n",
        "print(policy)\n",
        "\n",
        "# Reset it\n",
        "total_reward = 0.0\n",
        "state, _ = env.reset(seed=None)\n",
        "\n",
        "# While the episode is not finished\n",
        "terminated = False\n",
        "while not terminated:\n",
        "\n",
        "    # Use the policy to generate the probabilities of each action\n",
        "    probabilities = policy(state)\n",
        "\n",
        "    # ---> TODO: how to select an action\n",
        "    action = policy(state).argmax().item()\n",
        "\n",
        "    # One step forward\n",
        "    state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "    # Render (or not) the environment\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "\n",
        "# Print reward\n",
        "env.close()\n",
        "print(\"total_reward = {}\".format(total_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbe5941-23a9-464e-9e20-157582303e81",
      "metadata": {
        "id": "dbbe5941-23a9-464e-9e20-157582303e81"
      },
      "source": [
        "## 3) REINFORCE algorithm (/6)\n",
        "\n",
        "We want to find the parameters $\\theta$ that maximize the performance measure $J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[ G_0 ]$ with $G_t = \\sum_{k=0}^{\\infty} \\beta^k r_{t+k+1}$ and $\\beta \\in [0, 1]$ being a discount factor.\n",
        "To do so, we use the gradient ascent method: $\\theta_{k+1} = \\theta_{k} + \\alpha \\nabla_{\\theta_k} J(\\theta_k)$ with $\\alpha$ being the learning rate.\n",
        "The performance measure depends on both the action selection and the distribution of states.\n",
        "Both are affected by the policy parameters, which make the computation of the gradient challenging.\n",
        "\n",
        "The policy gradient theorem gives an expression for $\\nabla_\\theta J(\\theta)$ that does not involve the derivative of the state distribution.\n",
        "The expectation is over all possible state-action trajectories over the policy $\\pi_\\theta$:\n",
        "$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[ \\sum_{t=0}^{\\infty} G_t \\nabla_\\theta \\ln \\pi_\\theta(a_t | s_t) ]$.\n",
        "In the REINFORCE algorithm, we use a Monte-Carlo estimate over one episode, i.e., one trajectory:\n",
        "$\\nabla_\\theta J(\\theta) = \\sum_{t=0}^{\\infty} G_t \\nabla_\\theta \\ln \\pi_\\theta(a_t | s_t)$.\n",
        "\n",
        "Your objective is to complete the REINFORCE algorithm to train the policy until convergence. To solve the problem, you need to achieve a cumulative reward of at least 500 when training the policy. Below are: the code of the REINFORCE algorithm and a script to test your policy once it is trained.\n",
        "\n",
        "1.  Write the code to compute the discounted sum of rewards. (/1)\n",
        "2.  Write the REINFORCE loss for the policy. (/3)\n",
        "3.  Change the $\\alpha$ and the $\\beta$ parameters to solve the problem. (/1)\n",
        "4.  Find a good metric to decide when the training should be stopped. (/1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1nVKSvi4coTX",
      "metadata": {
        "id": "1nVKSvi4coTX"
      },
      "source": [
        "After playing around with the learning rate and discount factor, I set the learning rate to 0.001 and the discount factor to 0.99. As seen in how alpha and beta were set, I am biasing towards a slower but more stable convergence. Consistently, I see the first score of 500 around the 300th iteration, since reward tend to be more stable as more iterations occur, I decide to use iterations 1000 as a stopping point. However, if the training consistently yields rewards of 500 for 300 consecutive iterations, I also stop the training. Doing so consistently yields a high cumulative reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "935152bc-8c18-4767-8648-c671fe1dcaf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "935152bc-8c18-4767-8648-c671fe1dcaf1",
        "outputId": "fcc16a5c-03dc-4114-9f85-218953a8c1b2",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorModelV0(\n",
            "  (fc0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (policy): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n",
            "iteration 100 - last reward: 91.00\n",
            "iteration 200 - last reward: 195.00\n",
            "iteration 300 - last reward: 500.00\n",
            "iteration 400 - last reward: 500.00\n",
            "iteration 500 - last reward: 500.00\n",
            "iteration 600 - last reward: 500.00\n",
            "iteration 700 - last reward: 177.00\n",
            "iteration 800 - last reward: 500.00\n",
            "iteration 900 - last reward: 500.00\n",
            "iteration 1000 - last reward: 500.00\n"
          ]
        }
      ],
      "source": [
        "# Script to train a policy with the REINFORCE algorithm\n",
        "# ------------------------------------------\n",
        "\n",
        "# Maximum environment length\n",
        "HORIZON = 500\n",
        "\n",
        "# ---> TODO: change the discount factor to solve the problem\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "\n",
        "# ---> TODO: change the learning rate to solve the problem\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "best_reward = -float(\"inf\")\n",
        "\n",
        "# Create environment and policy\n",
        "env = CartpoleEnvV0()\n",
        "actor = ActorModelV0()\n",
        "actor_path = \"./actor_0.pt\"\n",
        "\n",
        "# Training mode\n",
        "actor.train()\n",
        "print(actor)\n",
        "\n",
        "# Create optimizer with the policy parameters\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ---> TODO: when do we stop the training?\n",
        "\n",
        "# Run infinitely many episodes\n",
        "training_iteration = 0\n",
        "while True:\n",
        "\n",
        "    # Experience\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Reset the environment\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    # During experience, we will save:\n",
        "    # - the probability of the chosen action at each time step pi(at|st)\n",
        "    # - the rewards received at each time step ri\n",
        "    saved_probabilities = list()\n",
        "    saved_rewards = list()\n",
        "\n",
        "    # Prevent infinite loop\n",
        "    for t in range(HORIZON + 1):\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "        # Use the policy to generate the probabilities of each action\n",
        "        probabilities = actor(state_tensor)\n",
        "\n",
        "        # Create a categorical distribution over the list of probabilities\n",
        "        # of actions and sample an action from it\n",
        "        distribution = Categorical(probabilities)\n",
        "        action = distribution.sample()\n",
        "\n",
        "        # Take the action\n",
        "        state, reward, terminated, _, _ = env.step(action.item())\n",
        "\n",
        "        # Save the probability of the chosen action and the reward\n",
        "        saved_probabilities.append(distribution.log_prob(action))\n",
        "        saved_rewards.append(reward)\n",
        "\n",
        "        # End episode\n",
        "        if terminated:\n",
        "            env.close()\n",
        "            break\n",
        "\n",
        "    # Compute discounted sum of rewards\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Current discounted reward\n",
        "    discounted_reward = 0.0\n",
        "\n",
        "    # List of all the discounted rewards, for each time step\n",
        "    discounted_rewards = list()\n",
        "\n",
        "    # ---> TODO: compute discounted rewards\n",
        "    # Why is the saved_rewards reversed?\n",
        "    for r in saved_rewards[::-1]:\n",
        "        discounted_reward = r + DISCOUNT_FACTOR * discounted_reward\n",
        "        discounted_rewards.insert(0, discounted_reward)\n",
        "\n",
        "    # Eventually normalize for stability purposes\n",
        "    discounted_rewards = torch.tensor(discounted_rewards)\n",
        "    mean, std = discounted_rewards.mean(), discounted_rewards.std()\n",
        "    discounted_rewards = (discounted_rewards - mean) / (std + 1e-7)\n",
        "\n",
        "    # Update policy parameters\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # For each time step\n",
        "    actor_loss = list()\n",
        "    for log_prob, g in zip(saved_probabilities, discounted_rewards):\n",
        "\n",
        "        # ---> TODO: compute policy loss\n",
        "        time_step_actor_loss = -g * log_prob\n",
        "\n",
        "        # Save it\n",
        "        actor_loss.append(time_step_actor_loss)\n",
        "\n",
        "    # Sum all the time step losses\n",
        "    actor_loss = torch.cat(actor_loss).sum()\n",
        "\n",
        "    # Reset gradients to 0.0\n",
        "    actor_optimizer.zero_grad()\n",
        "\n",
        "    # Compute the gradients of the loss (backpropagation)\n",
        "    actor_loss.backward()\n",
        "\n",
        "    # Update the policy parameters (gradient ascent)\n",
        "    actor_optimizer.step()\n",
        "\n",
        "    # Logging\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Episode total reward\n",
        "    episode_total_reward = sum(saved_rewards)\n",
        "\n",
        "    recent_rewards = []\n",
        "\n",
        "    # Log results\n",
        "    log_frequency = 5\n",
        "    training_iteration += 1\n",
        "    if training_iteration % log_frequency == 0:\n",
        "      if training_iteration <= 1000:\n",
        "\n",
        "        # Save neural network\n",
        "        torch.save(actor, actor_path)\n",
        "        recent_rewards.append(episode_total_reward)\n",
        "\n",
        "        if len(recent_rewards) > 200:\n",
        "          recent_rewards.pop(0)\n",
        "\n",
        "        # Stopping Criteria: if the previous 200 ones are all 500, stop.\n",
        "        if (len(recent_rewards) == 200 and\n",
        "            all(r >= 500 for r in recent_rewards[-200:]) and\n",
        "            all(r >= 500 for r in recent_rewards[-100:]) and\n",
        "            all(r >= 500 for r in recent_rewards[-50:])):\n",
        "            print(\"Achieved sustained high performance - stopping training\")\n",
        "            break\n",
        "\n",
        "        # Print results\n",
        "        if training_iteration % 100 == 0:\n",
        "          print(\"iteration {} - last reward: {:.2f}\".format(\n",
        "              training_iteration, episode_total_reward))\n",
        "      else:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d35becdf-4a22-4b5a-87ab-f148a4e90456",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35becdf-4a22-4b5a-87ab-f148a4e90456",
        "outputId": "04007776-0130-4082-a9bf-7792835cf73a",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorModelV0(\n",
            "  (fc0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (policy): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1488ed516cef>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  policy = torch.load(actor_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_reward = 500.0\n"
          ]
        }
      ],
      "source": [
        "# Script to run a deep neural network policy (after training)\n",
        "# ------------------------------------------\n",
        "\n",
        "# Create environment and policy\n",
        "env = CartpoleEnvV0()\n",
        "policy = ActorModelV0()\n",
        "actor_path = \"./actor_0.pt\"\n",
        "\n",
        "# Testing mode\n",
        "policy.eval()\n",
        "print(policy)\n",
        "\n",
        "# Load the trained policy\n",
        "policy = torch.load(actor_path)\n",
        "\n",
        "# Reset it\n",
        "total_reward = 0.0\n",
        "state, _ = env.reset(seed=None)\n",
        "\n",
        "# While the episode is not finished\n",
        "terminated = False\n",
        "while not terminated:\n",
        "\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "    # Use the policy to generate the probabilities of each action\n",
        "    probabilities = policy(state_tensor)\n",
        "\n",
        "    # Create distribution and sample action\n",
        "    distribution = Categorical(probabilities)\n",
        "    action = distribution.sample().item()\n",
        "\n",
        "    # One step forward\n",
        "    state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "    # Render (or not) the environment\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "\n",
        "# Print reward\n",
        "env.close()\n",
        "print(\"total_reward = {}\".format(total_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1e5416-d3aa-469c-b37a-a7d5e6275144",
      "metadata": {
        "id": "9b1e5416-d3aa-469c-b37a-a7d5e6275144"
      },
      "source": [
        "## 4) Markov property (/5)\n",
        "\n",
        "Usually, in reinforcement learning, the state can be decomposed in 2 objects: the *state* describing fully the environment and the *observation*, describing what an agent (i.e., a policy) can observe from the state.\n",
        "Until here, the state and the observation are equal.\n",
        "In this section, we work on a slightly different version of the environment where the state is different from the observation: the accelerations are removed, and are therefore, not visible by the agent.\n",
        "Belore are: the new cartpole environment, a new actor and a new REINFORCE script.\n",
        "1) Because the observation size is then of size 2, the actor is redefined accordingly.\n",
        "Run the REINFORCE algorithm back again with the refedined environment and actor.\n",
        "Does it converge?\n",
        "Why? (/1)\n",
        "2) Find a way to modify the states in the new environment, without using the accelerations, so that the environment becomes Markovian again.\n",
        "Update the actor network accordingly and train the model back again with the REINFORCE algorithm.\n",
        "Does it converge?\n",
        "Why? (/4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12a12761-c1f4-435f-a052-c1cd7c5e5bf9",
      "metadata": {
        "id": "12a12761-c1f4-435f-a052-c1cd7c5e5bf9"
      },
      "source": [
        "1. No, the REINFORCE algorithm does not converge in this case because it isn't markovian\n",
        "\n",
        "2. We've reinstated the markovian decision process by introducing the last known location and angle of the pole, thus hoping that the model will learn the velocity. The model now converges, but at a much slower speed with much more iterations. (x_t-1, x, theta_t-1, theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58871444-5241-4bc5-aa63-20682d444e83",
      "metadata": {
        "id": "58871444-5241-4bc5-aa63-20682d444e83",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\"\"\"Cartpole environment (modified).\"\"\"\n",
        "\n",
        "class CartpoleEnvV1(gym.Env):\n",
        "    \"\"\"Cartpole environment.\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
        "\n",
        "    def __init__(self, env_context=None, render_mode=None):\n",
        "        \"\"\"Initialize environment.\n",
        "\n",
        "        Args:\n",
        "            env_context (dict): environment configuration.\n",
        "            render_mode (str): render mode.\n",
        "        \"\"\"\n",
        "        # Variables\n",
        "        self.gravity = 9.8\n",
        "        self.masscart = 1.0\n",
        "        self.masspole = 0.1\n",
        "        self.total_mass = self.masspole + self.masscart\n",
        "        self.length = 0.5  # Actually half the pole's length\n",
        "        self.polemass_length = self.masspole * self.length\n",
        "        self.force_mag = 10.0\n",
        "        self.tau = 0.02  # Seconds between state updates\n",
        "        self.kinematics_integrator = \"euler\"\n",
        "\n",
        "        # Angle at which to fail the episode\n",
        "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
        "        self.x_threshold = 2.4\n",
        "\n",
        "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
        "        # is still within bounds\n",
        "        high = np.array([\n",
        "            self.x_threshold * 2,\n",
        "            np.finfo(np.float32).max,\n",
        "            self.theta_threshold_radians * 2,\n",
        "            np.finfo(np.float32).max,\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # Action and observation (state) spaces\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
        "\n",
        "        # Render mode\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Others\n",
        "        self.screen_width = 600\n",
        "        self.screen_height = 400\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "        self.state = None\n",
        "\n",
        "    def _process_state(self):\n",
        "        \"\"\"Process state before returning it.\n",
        "\n",
        "        Returns:\n",
        "            state_processed (numpy.array): processed state.\n",
        "        \"\"\"\n",
        "        # Accelerations are removed from the state\n",
        "        #processed_state = np.array([self.state[0], self.state[2]])\n",
        "\n",
        "        # ---> TODO: if no accelerations, determine a new working state\n",
        "        #I'm using the failure threshold, the angle of the pole, and the threshold for the distance of fail, while penalizing further angles disproportionally\n",
        "        #stability_index = (abs(self.state[2]) / self.theta_threshold_radians)**2 + (abs(self.state[0]) / self.x_threshold)\n",
        "\n",
        "        #(x_t-1, x, theta_t-1, theta)\n",
        "        prev_x = self.state[0] - self.tau * self.state[1]\n",
        "        prev_theta = self.state[2] - self.tau * self.state[3]\n",
        "\n",
        "        processed_state = np.array([prev_x, self.state[0], prev_theta, self.state[2]])\n",
        "\n",
        "        return processed_state\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment.\n",
        "\n",
        "        Args:\n",
        "            seed (int): seed for reproducibility.\n",
        "            options (dict): additional information.\n",
        "\n",
        "        Returns:\n",
        "            state (numpy.array): the processed state.\n",
        "\n",
        "            info (dict): auxiliary diagnostic information.\n",
        "        \"\"\"\n",
        "        # Reset seed\n",
        "        if seed is not None:\n",
        "            self._np_random, seed = seeding.np_random(seed)\n",
        "\n",
        "        # Current time step\n",
        "        self._time_step = 0\n",
        "\n",
        "        # Reset state\n",
        "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
        "        self.state = self.state.astype(np.float32)\n",
        "\n",
        "        # ---> TODO: if no accelerations, determine a new working state\n",
        "        #initial_theta = self.state[2]\n",
        "        #initial_x = self.state[0]\n",
        "        #initial_stability = (abs(initial_theta) / self.theta_threshold_radians)**2 + (abs(initial_x) / self.x_threshold)\n",
        "\n",
        "        # Eventually render\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return self._process_state(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Go from current step to next one.\n",
        "\n",
        "        Args:\n",
        "            action (int): action of the agent.\n",
        "\n",
        "        Returns:\n",
        "            state (numpy.array): state.\n",
        "\n",
        "            reward (float): reward.\n",
        "\n",
        "            terminated (bool): whether a terminal state is reached.\n",
        "\n",
        "            truncated (bool): whether a truncation condition is reached.\n",
        "\n",
        "            info (dict): auxiliary diagnostic information.\n",
        "        \"\"\"\n",
        "        # Check if action is valid\n",
        "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
        "        assert self.action_space.contains(action), err_msg\n",
        "        assert self.state is not None, \"Call reset before using step method.\"\n",
        "\n",
        "        # Compute variables\n",
        "        x_tmp = self.state\n",
        "        x, x_dot, theta, theta_dot = x_tmp[0], x_tmp[1], x_tmp[2], x_tmp[3]\n",
        "        force = self.force_mag if action == 1 else -self.force_mag\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "\n",
        "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
        "        m = self.polemass_length\n",
        "        temp = force + m * theta_dot**2 * sintheta / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta * temp)\n",
        "        thetaacc /= 4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass\n",
        "        thetaacc /= self.length\n",
        "        xacc = temp - m * thetaacc * costheta / self.total_mass\n",
        "\n",
        "        # Update system\n",
        "        if self.kinematics_integrator == \"euler\":\n",
        "            x = x + self.tau * x_dot\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "        else:  # Semi-implicit euler\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            x = x + self.tau * x_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "\n",
        "        # ---> TODO: if no accelerations, determine a new working state\n",
        "\n",
        "        # Full system state\n",
        "        self.state = np.array([\n",
        "            x,\n",
        "            x_dot,\n",
        "            theta,\n",
        "            theta_dot,\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # Reward is 1\n",
        "        reward = 1.0\n",
        "\n",
        "        # Increase time step\n",
        "        self._time_step += 1\n",
        "\n",
        "        # Check if episode if finished\n",
        "        terminated = bool(\n",
        "            x < -self.x_threshold\n",
        "            or x > self.x_threshold\n",
        "            or theta < -self.theta_threshold_radians\n",
        "            or theta > self.theta_threshold_radians\n",
        "            or self._time_step >= 500\n",
        "        )\n",
        "\n",
        "        # Eventually render\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return self._process_state(), reward, terminated, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render environment.\n",
        "\n",
        "        Note:\n",
        "            Do not pay too much attention to this function. It is just to\n",
        "            display a nice animation of the environment.\n",
        "        \"\"\"\n",
        "        import pygame\n",
        "        from pygame import gfxdraw\n",
        "\n",
        "        # Initialize render mode if needed\n",
        "        if self.render_mode is None:\n",
        "            self.render_mode = \"human\"\n",
        "\n",
        "        # Initialize objects\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            if self.render_mode == \"human\":\n",
        "                pygame.display.init()\n",
        "                self.screen = pygame.display.set_mode(\n",
        "                    (self.screen_width, self.screen_height))\n",
        "            else:  # mode == \"rgb_array\"\n",
        "                self.screen = pygame.Surface(\n",
        "                    (self.screen_width, self.screen_height))\n",
        "\n",
        "        # Initialize clock\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        # Objects\n",
        "        world_width = self.x_threshold * 2\n",
        "        scale = self.screen_width / world_width\n",
        "        polewidth = 10.0\n",
        "        polelen = scale * (2 * self.length)\n",
        "        cartwidth = 50.0\n",
        "        cartheight = 30.0\n",
        "\n",
        "        # Get state\n",
        "        if self.state is None:\n",
        "            return None\n",
        "        x = self.state\n",
        "\n",
        "        # Get surface\n",
        "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
        "        self.surf.fill((255, 255, 255))\n",
        "\n",
        "        # Computations\n",
        "        l = -cartwidth / 2\n",
        "        r = cartwidth / 2\n",
        "        t = cartheight / 2\n",
        "        b = -cartheight / 2\n",
        "        axleoffset = cartheight / 4.0\n",
        "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
        "        carty = 100  # TOP OF CART\n",
        "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
        "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
        "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
        "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
        "\n",
        "        l, r, t, b = (\n",
        "            -polewidth / 2,\n",
        "            polewidth / 2,\n",
        "            polelen - polewidth / 2,\n",
        "            -polewidth / 2,\n",
        "        )\n",
        "\n",
        "        pole_coords = []\n",
        "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
        "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
        "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
        "            pole_coords.append(coord)\n",
        "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
        "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
        "\n",
        "        gfxdraw.aacircle(self.surf,\n",
        "                         int(cartx),\n",
        "                         int(carty + axleoffset),\n",
        "                         int(polewidth / 2),\n",
        "                         (129, 132, 203))\n",
        "        gfxdraw.filled_circle(self.surf,\n",
        "                              int(cartx),\n",
        "                              int(carty + axleoffset),\n",
        "                              int(polewidth / 2),\n",
        "                              (129, 132, 203))\n",
        "\n",
        "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
        "\n",
        "        # Display\n",
        "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
        "        self.screen.blit(self.surf, (0, 0))\n",
        "\n",
        "        # Human mode\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            pygame.display.flip()\n",
        "\n",
        "        # RGB array mode\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)),\n",
        "                axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the environment.\n",
        "\n",
        "        Note:\n",
        "            Do not pay too much attention to this function. It is just to close\n",
        "            the environment.\n",
        "        \"\"\"\n",
        "        if self.screen is not None:\n",
        "            import pygame\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.isopen = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd755cf3-f663-465f-8467-620754aa9d07",
      "metadata": {
        "id": "dd755cf3-f663-465f-8467-620754aa9d07",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\"\"\"Actor network (modified).\"\"\"\n",
        "\n",
        "class ActorModelV1(nn.Module):\n",
        "    \"\"\"Deep neural network.\"\"\"\n",
        "\n",
        "    # By default, use CPU\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize model.\"\"\"\n",
        "        super(ActorModelV1, self).__init__()\n",
        "        # ---> TODO: change input and output sizes depending on the environment\n",
        "        input_size = 4\n",
        "        nb_actions = 2\n",
        "\n",
        "        # Build layer objects\n",
        "        self.fc0 = nn.Linear(input_size, 128)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.policy = nn.Linear(128, nb_actions)\n",
        "\n",
        "    def _preprocessor(self, state):\n",
        "        \"\"\"Preprocessor function.\n",
        "\n",
        "        Args:\n",
        "            state (numpy.array): environment state.\n",
        "\n",
        "        Returns:\n",
        "            x (torch.tensor): preprocessed state.\n",
        "        \"\"\"\n",
        "        # Add batch dimension\n",
        "        x = np.expand_dims(state, 0)\n",
        "\n",
        "        # Transform to torch.tensor\n",
        "        x = torch.from_numpy(x).float().to(self.DEVICE)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (numpy.array): environment state.\n",
        "\n",
        "        Returns:\n",
        "            actions_prob (torch.tensor): list with the probability of each\n",
        "                action over the action space.\n",
        "        \"\"\"\n",
        "        # Preprocessor\n",
        "        x = self._preprocessor(x)\n",
        "\n",
        "        # Input layer\n",
        "        x = F.relu(self.fc0(x))\n",
        "\n",
        "        # Middle layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Policy\n",
        "        action_prob = F.softmax(self.policy(x), dim=-1)\n",
        "\n",
        "        return action_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42fb13f9-6f09-4c1d-beb3-755b46a6e220",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42fb13f9-6f09-4c1d-beb3-755b46a6e220",
        "scrolled": true,
        "collapsed": true,
        "outputId": "7d73a27d-9803-4269-e316-069ca301c684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorModelV1(\n",
            "  (fc0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (policy): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n",
            "iteration 100 - last reward: 19.00\n",
            "iteration 200 - last reward: 13.00\n",
            "iteration 300 - last reward: 16.00\n",
            "iteration 400 - last reward: 59.00\n",
            "iteration 500 - last reward: 39.00\n",
            "iteration 600 - last reward: 48.00\n",
            "iteration 700 - last reward: 32.00\n",
            "iteration 800 - last reward: 26.00\n",
            "iteration 900 - last reward: 41.00\n",
            "iteration 1000 - last reward: 12.00\n",
            "iteration 1100 - last reward: 52.00\n",
            "iteration 1200 - last reward: 31.00\n",
            "iteration 1300 - last reward: 23.00\n",
            "iteration 1400 - last reward: 13.00\n",
            "iteration 1500 - last reward: 45.00\n",
            "iteration 1600 - last reward: 38.00\n",
            "iteration 1700 - last reward: 36.00\n",
            "iteration 1800 - last reward: 30.00\n",
            "iteration 1900 - last reward: 88.00\n",
            "iteration 2000 - last reward: 44.00\n",
            "iteration 2100 - last reward: 10.00\n",
            "iteration 2200 - last reward: 45.00\n",
            "iteration 2300 - last reward: 52.00\n",
            "iteration 2400 - last reward: 10.00\n",
            "iteration 2500 - last reward: 52.00\n",
            "iteration 2600 - last reward: 58.00\n",
            "iteration 2700 - last reward: 54.00\n",
            "iteration 2800 - last reward: 38.00\n",
            "iteration 2900 - last reward: 10.00\n",
            "iteration 3000 - last reward: 303.00\n",
            "iteration 3100 - last reward: 119.00\n",
            "iteration 3200 - last reward: 64.00\n",
            "iteration 3300 - last reward: 106.00\n",
            "iteration 3400 - last reward: 93.00\n",
            "iteration 3500 - last reward: 87.00\n",
            "iteration 3600 - last reward: 393.00\n",
            "iteration 3700 - last reward: 10.00\n",
            "iteration 3800 - last reward: 9.00\n",
            "iteration 3900 - last reward: 13.00\n",
            "iteration 4000 - last reward: 500.00\n",
            "iteration 4100 - last reward: 500.00\n",
            "iteration 4200 - last reward: 11.00\n",
            "iteration 4300 - last reward: 154.00\n",
            "iteration 4400 - last reward: 90.00\n",
            "iteration 4500 - last reward: 49.00\n",
            "iteration 4600 - last reward: 500.00\n",
            "iteration 4700 - last reward: 26.00\n",
            "iteration 4800 - last reward: 11.00\n",
            "iteration 4900 - last reward: 115.00\n",
            "iteration 5000 - last reward: 120.00\n",
            "iteration 5100 - last reward: 11.00\n",
            "iteration 5200 - last reward: 11.00\n",
            "iteration 5300 - last reward: 162.00\n",
            "iteration 5400 - last reward: 111.00\n",
            "iteration 5500 - last reward: 122.00\n",
            "iteration 5600 - last reward: 275.00\n",
            "iteration 5700 - last reward: 500.00\n",
            "iteration 5800 - last reward: 11.00\n",
            "iteration 5900 - last reward: 12.00\n",
            "iteration 6000 - last reward: 9.00\n",
            "iteration 6100 - last reward: 10.00\n",
            "iteration 6200 - last reward: 488.00\n",
            "iteration 6300 - last reward: 114.00\n",
            "iteration 6400 - last reward: 98.00\n",
            "iteration 6500 - last reward: 500.00\n",
            "iteration 6600 - last reward: 500.00\n",
            "iteration 6700 - last reward: 10.00\n",
            "iteration 6800 - last reward: 123.00\n",
            "iteration 6900 - last reward: 11.00\n",
            "iteration 7000 - last reward: 10.00\n",
            "iteration 7100 - last reward: 106.00\n",
            "iteration 7200 - last reward: 289.00\n",
            "iteration 7300 - last reward: 500.00\n",
            "iteration 7400 - last reward: 156.00\n",
            "iteration 7500 - last reward: 39.00\n",
            "iteration 7600 - last reward: 179.00\n",
            "iteration 7700 - last reward: 500.00\n",
            "iteration 7800 - last reward: 88.00\n",
            "iteration 7900 - last reward: 121.00\n",
            "iteration 8000 - last reward: 89.00\n",
            "iteration 8100 - last reward: 192.00\n",
            "iteration 8200 - last reward: 154.00\n",
            "iteration 8300 - last reward: 260.00\n",
            "iteration 8400 - last reward: 160.00\n",
            "iteration 8500 - last reward: 155.00\n",
            "iteration 8600 - last reward: 172.00\n",
            "iteration 8700 - last reward: 12.00\n",
            "iteration 8800 - last reward: 58.00\n",
            "iteration 8900 - last reward: 149.00\n",
            "iteration 9000 - last reward: 117.00\n",
            "iteration 9100 - last reward: 95.00\n",
            "iteration 9200 - last reward: 144.00\n",
            "iteration 9300 - last reward: 500.00\n",
            "iteration 9400 - last reward: 143.00\n",
            "iteration 9500 - last reward: 500.00\n",
            "iteration 9600 - last reward: 500.00\n",
            "iteration 9700 - last reward: 500.00\n",
            "iteration 9800 - last reward: 500.00\n",
            "iteration 9900 - last reward: 500.00\n",
            "iteration 10000 - last reward: 500.00\n"
          ]
        }
      ],
      "source": [
        "# Script to train a policy with the REINFORCE algorithm (modified)\n",
        "# ------------------------------------------\n",
        "\n",
        "# Maximum environment length\n",
        "HORIZON = 500\n",
        "\n",
        "# ---> TODO: change the discount factor to solve the problem\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "\n",
        "# ---> TODO: change the learning rate to solve the problem\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Create environment and policy\n",
        "env = CartpoleEnvV1()\n",
        "actor = ActorModelV1()\n",
        "actor_path = \"./actor_1.pt\"\n",
        "\n",
        "# Training mode\n",
        "actor.train()\n",
        "print(actor)\n",
        "\n",
        "# Create optimizer with the policy parameters\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ---> TODO: when do we stop the training?\n",
        "\n",
        "# Run infinitely many episodes\n",
        "training_iteration = 0\n",
        "while True:\n",
        "\n",
        "    # Experience\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Reset the environment\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    # During experience, we will save:\n",
        "    # - the probability of the chosen action at each time step pi(at|st)\n",
        "    # - the rewards received at each time step ri\n",
        "    saved_probabilities = list()\n",
        "    saved_rewards = list()\n",
        "\n",
        "    # Prevent infinite loop\n",
        "    for t in range(HORIZON + 1):\n",
        "\n",
        "        # Use the policy to generate the probabilities of each action\n",
        "        probabilities = actor(state)\n",
        "\n",
        "        # Create a categorical distribution over the list of probabilities\n",
        "        # of actions and sample an action from it\n",
        "        distribution = Categorical(probabilities)\n",
        "        action = distribution.sample()\n",
        "\n",
        "        # Take the action\n",
        "        state, reward, terminated, _, _ = env.step(action.item())\n",
        "\n",
        "        # Save the probability of the chosen action and the reward\n",
        "        saved_probabilities.append(probabilities[0][action])\n",
        "        saved_rewards.append(reward)\n",
        "\n",
        "        # End episode\n",
        "        if terminated:\n",
        "            env.close()\n",
        "            break\n",
        "\n",
        "    # Compute discounted sum of rewards\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Current discounted reward\n",
        "    discounted_reward = 0.0\n",
        "\n",
        "    # List of all the discounted rewards, for each time step\n",
        "    discounted_rewards = list()\n",
        "\n",
        "    # ---> TODO: compute discounted rewards\n",
        "    for r in saved_rewards[::-1]:\n",
        "        discounted_reward = r + DISCOUNT_FACTOR * discounted_reward\n",
        "        discounted_rewards.insert(0, discounted_reward)\n",
        "\n",
        "    # Eventually normalize for stability purposes\n",
        "    discounted_rewards = torch.tensor(discounted_rewards)\n",
        "    mean, std = discounted_rewards.mean(), discounted_rewards.std()\n",
        "    discounted_rewards = (discounted_rewards - mean) / (std + 1e-7)\n",
        "\n",
        "    # Update policy parameters\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # For each time step\n",
        "    actor_loss = list()\n",
        "    for p, g in zip(saved_probabilities, discounted_rewards):\n",
        "\n",
        "        # ---> TODO: compute policy loss\n",
        "        time_step_actor_loss = -g * torch.log(p)\n",
        "\n",
        "        # Save it\n",
        "        actor_loss.append(time_step_actor_loss)\n",
        "\n",
        "    # Sum all the time step losses\n",
        "    actor_loss = torch.cat(actor_loss).sum()\n",
        "\n",
        "    # Reset gradients to 0.0\n",
        "    actor_optimizer.zero_grad()\n",
        "\n",
        "    # Compute the gradients of the loss (backpropagation)\n",
        "    actor_loss.backward()\n",
        "\n",
        "    # Update the policy parameters (gradient ascent)\n",
        "    actor_optimizer.step()\n",
        "\n",
        "    # Logging\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Episode total reward\n",
        "    episode_total_reward = sum(saved_rewards)\n",
        "\n",
        "    # ---> TODO: when do we stop the training?\n",
        "\n",
        "    # Log results\n",
        "    log_frequency = 5\n",
        "    training_iteration += 1\n",
        "    if training_iteration % log_frequency == 0:\n",
        "      if training_iteration <= 10000:\n",
        "\n",
        "        # Save neural network\n",
        "        torch.save(actor, actor_path)\n",
        "        recent_rewards.append(episode_total_reward)\n",
        "\n",
        "        if len(recent_rewards) > 200:\n",
        "          recent_rewards.pop(0)\n",
        "\n",
        "        # Stopping Criteria: if the previous 200 ones are all 500, stop.\n",
        "        if (len(recent_rewards) == 200 and\n",
        "            all(r >= 400 for r in recent_rewards[-200:]) and\n",
        "            all(r >= 400 for r in recent_rewards[-100:]) and\n",
        "            all(r >= 400 for r in recent_rewards[-50:])):\n",
        "            print(\"Achieved sustained high performance - stopping training\")\n",
        "            break\n",
        "\n",
        "        # Print results\n",
        "        if training_iteration % 100 == 0:\n",
        "          print(\"iteration {} - last reward: {:.2f}\".format(\n",
        "              training_iteration, episode_total_reward))\n",
        "      else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d636435-4c1b-4e97-9c7d-6918289b24eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d636435-4c1b-4e97-9c7d-6918289b24eb",
        "outputId": "59af1df9-9ef1-44e6-a99c-4bb548ab3178",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorModelV1(\n",
            "  (fc0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (policy): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-ea6456cb3993>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  policy = torch.load(actor_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_reward = 500.0\n"
          ]
        }
      ],
      "source": [
        "# Script to run a deep neural network policy (after training, modified)\n",
        "# ------------------------------------------\n",
        "\n",
        "# Create environment and policy\n",
        "env = CartpoleEnvV1()\n",
        "policy = ActorModelV1()\n",
        "actor_path = \"./actor_1.pt\"\n",
        "\n",
        "# Testing mode\n",
        "policy.eval()\n",
        "print(policy)\n",
        "\n",
        "# Load the trained policy\n",
        "policy = torch.load(actor_path)\n",
        "\n",
        "# Reset it\n",
        "total_reward = 0.0\n",
        "state, _ = env.reset(seed=None)\n",
        "\n",
        "# While the episode is not finished\n",
        "terminated = False\n",
        "while not terminated:\n",
        "\n",
        "    # Use the policy to generate the probabilities of each action\n",
        "    probabilities = policy(state)\n",
        "\n",
        "    # ---> TODO: how to select an action\n",
        "    distribution = Categorical(probabilities)\n",
        "    action = distribution.sample().item()\n",
        "\n",
        "    # One step forward\n",
        "    state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "    # Render (or not) the environment\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "\n",
        "# Print reward\n",
        "env.close()\n",
        "print(\"total_reward = {}\".format(total_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9958e1-bc22-4355-ab00-a0ca8afeef5e",
      "metadata": {
        "id": "bc9958e1-bc22-4355-ab00-a0ca8afeef5e"
      },
      "source": [
        "## 5) Open questions (/5)\n",
        "\n",
        "You can choose to work on one or several of the following subjects.\n",
        "You are also encouraged to work on other subjects, if you have any ideas.\n",
        "Do not hesitate to ask to get the validation for a subject not listed here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a5a7c8d-5ef9-454c-b0f0-8d118784a76d",
      "metadata": {
        "id": "7a5a7c8d-5ef9-454c-b0f0-8d118784a76d"
      },
      "source": [
        "### Different environments (moderately easy)\n",
        "You can find other reinforcement learning problems with their corresponding environments and adapt your REINFORCE training script and your actor model to solve it.\n",
        "You could also create your own simple reinforcement learning problem, determine the sates, the actions, the rewards and code your own environment to solve it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f949da12-233d-4899-836f-e719f674e99f",
      "metadata": {
        "id": "f949da12-233d-4899-836f-e719f674e99f"
      },
      "source": [
        "**Your answer**: TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "646be205-1adc-40c3-9142-03d36ee50238",
      "metadata": {
        "id": "646be205-1adc-40c3-9142-03d36ee50238"
      },
      "source": [
        "### REINFORCE with baseline (moderately difficult)\n",
        "To reduce the variance of the REINFORCE method, we can subtract to $G_t$ a baseline $v(s_t)$.\n",
        "With $v(s)$ being the estimate of the state value given by another neural-network, called critic.\n",
        "Based on the actor network, build the network for the critic; code are quasi-similar (the outputs are different).\n",
        "And based on the REINFORCE training script, build the REINFORCE with baseline training script."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8bf741-b283-496b-92d9-e544e2d84074",
      "metadata": {
        "id": "ff8bf741-b283-496b-92d9-e544e2d84074"
      },
      "source": [
        "**Your answer**: TODO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa853dc6-4ba1-42ed-bf8b-73440895e77e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "aa853dc6-4ba1-42ed-bf8b-73440895e77e",
        "outputId": "69fd3b63-6664-4bd6-c96f-88e16597497a",
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'CriticModel' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-48881fafd599>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartpoleEnvV0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorModelV0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCriticModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mactor_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./actor_2.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcritic_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./critic_2.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CriticModel' is not defined"
          ]
        }
      ],
      "source": [
        "# Script to train a policy with the REINFORCE with baseline algorithm\n",
        "# ------------------------------------------\n",
        "\n",
        "# Maximum environment length\n",
        "HORIZON = 500\n",
        "\n",
        "# ---> TODO: change the discount factor to solve the problem\n",
        "DISCOUNT_FACTOR = 0.1\n",
        "\n",
        "# ---> TODO: change the learning rate to solve the problem\n",
        "LEARNING_RATE = 0.5\n",
        "\n",
        "# Create environment, policy and critic\n",
        "env = CartpoleEnvV0()\n",
        "actor = ActorModelV0()\n",
        "critic = CriticModel()\n",
        "actor_path = \"./actor_2.pt\"\n",
        "critic_path = \"./critic_2.pt\"\n",
        "\n",
        "# Training mode\n",
        "actor.train()\n",
        "critic.train()\n",
        "print(actor)\n",
        "print(critic)\n",
        "\n",
        "# Create optimizer with the policy parameters\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Create optimizer with the critic parameters\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ---> TODO: based on the REINFORCE script, create the REINFORCE with\n",
        "# baseline script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b780738-ca39-426e-8bbd-301776a0a6df",
      "metadata": {
        "id": "9b780738-ca39-426e-8bbd-301776a0a6df"
      },
      "outputs": [],
      "source": [
        "# Script to test the policy after training\n",
        "\n",
        "# Create policy\n",
        "policy = ActorModelV0()\n",
        "policy.eval()\n",
        "print(policy)\n",
        "\n",
        "# Load the trained policy\n",
        "policy = torch.load(\"./actor_2.pt\")\n",
        "\n",
        "# Create environment\n",
        "env = CartpoleEnvV0()\n",
        "\n",
        "# Reset it\n",
        "total_reward = 0.0\n",
        "state, _ = env.reset(seed=None)\n",
        "\n",
        "# While the episode is not finished\n",
        "terminated = False\n",
        "while not terminated:\n",
        "\n",
        "    # Use the policy to generate the probabilities of each action\n",
        "    probabilities = policy(state)\n",
        "\n",
        "    # ---> TODO: how to select an action\n",
        "    action = 0\n",
        "\n",
        "    # One step forward\n",
        "    state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "    # Render (or not) the environment\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "\n",
        "# Print reward\n",
        "env.close()\n",
        "print(\"total_reward = {}\".format(total_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7d98e2a-a1e2-40fc-955c-1d8233cbc76a",
      "metadata": {
        "id": "a7d98e2a-a1e2-40fc-955c-1d8233cbc76a"
      },
      "source": [
        "### Actor-critic algorithm (moderately difficult)\n",
        "\n",
        "I built a critic network similar to the actor network, with an identical architecture. The key difference is the output where output is V, a single value representing state value.   \n",
        "\n",
        "Critic loss is calculated by mean squared loss to help the critic learn to predict expected returns. This training process runs parallel to the actor training process and the critic's predictions are used to compute advantages for the actor where Advantages = Returns - Value predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0d3183a-f57b-4a7f-8385-00e48cd1f2ef",
      "metadata": {
        "id": "a0d3183a-f57b-4a7f-8385-00e48cd1f2ef"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb4f35a9-1d4d-41d8-ac41-6edaaa1b279c",
      "metadata": {
        "id": "eb4f35a9-1d4d-41d8-ac41-6edaaa1b279c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# ---> TODO: based on the actor network, build a critic network\n",
        "\n",
        "class CriticModel(nn.Module):\n",
        "    \"\"\"Critic network to estimate the value function V(s).\"\"\"\n",
        "\n",
        "    # By default, use CPU\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize model.\"\"\"\n",
        "        super(CriticModel, self).__init__()\n",
        "        # Same input size as actor (state space dimension)\n",
        "        input_size = 4\n",
        "        # Output size is 1 since we're estimating a single value V(s)\n",
        "        output_size = 1\n",
        "\n",
        "        # Build layer objects - using same architecture to actor\n",
        "        self.fc0 = nn.Linear(input_size, 128)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.value = nn.Linear(128, output_size)\n",
        "\n",
        "    def _preprocessor(self, state):\n",
        "        \"\"\"Preprocessor function.\n",
        "\n",
        "        Args:\n",
        "            state (numpy.array): environment state.\n",
        "\n",
        "        Returns:\n",
        "            x (torch.tensor): preprocessed state.\n",
        "        \"\"\"\n",
        "        # Add batch dimension if necessary\n",
        "        x = np.expand_dims(state, 0)\n",
        "\n",
        "        # Transform to torch.tensor\n",
        "        x = torch.from_numpy(x).float().to(self.DEVICE)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (numpy.array): environment state.\n",
        "\n",
        "        Returns:\n",
        "            state_value (torch.tensor): estimated value V(s) for the input state.\n",
        "        \"\"\"\n",
        "        # Preprocessor\n",
        "        x = self._preprocessor(x)\n",
        "\n",
        "        # Input layer\n",
        "        x = F.relu(self.fc0(x))\n",
        "\n",
        "        # Middle layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Value estimation (no activation function needed since we're predicting a scalar value)\n",
        "        state_value = self.value(x)\n",
        "\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3836bc4a-bcc8-4b62-adcc-df0eda8f032d",
      "metadata": {
        "id": "3836bc4a-bcc8-4b62-adcc-df0eda8f032d",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "outputId": "50400eb1-3cb9-433c-d586-6cc3417b43bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorModelV0(\n",
            "  (fc0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (policy): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n",
            "CriticModel(\n",
            "  (fc0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (value): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-163f5b738287>:92: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  time_step_critic_loss = F.mse_loss(v, torch.tensor([r]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100 - Last reward: 24.00\n",
            "Actor loss: -0.3491, Critic loss: 1167.0490\n",
            "Iteration 200 - Last reward: 172.00\n",
            "Actor loss: -3.9792, Critic loss: 96534.5312\n",
            "Iteration 300 - Last reward: 500.00\n",
            "Actor loss: -10.3636, Critic loss: 123325.5625\n",
            "Iteration 400 - Last reward: 500.00\n",
            "Actor loss: -10.7415, Critic loss: 245413.6562\n",
            "Iteration 500 - Last reward: 500.00\n",
            "Actor loss: 7.2687, Critic loss: 181930.6875\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-163f5b738287>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mprobs_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0msamples_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Script to train a policy with the actor-critic algorithm\n",
        "# ------------------------------------------\n",
        "\n",
        "# Maximum environment length\n",
        "HORIZON = 500\n",
        "\n",
        "# ---> TODO: change the discount factor to solve the problem\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "\n",
        "# ---> TODO: change the learning rate to solve the problem\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Create environment, policy and critic\n",
        "env = CartpoleEnvV0()\n",
        "actor = ActorModelV0()\n",
        "critic = CriticModel()\n",
        "actor_path = \"./actor_3.pt\"\n",
        "critic_path = \"./critic_3.pt\"\n",
        "\n",
        "# Training mode\n",
        "actor.train()\n",
        "critic.train()\n",
        "print(actor)\n",
        "print(critic)\n",
        "\n",
        "# Create optimizer with the policy parameters\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Create optimizer with the critic parameters\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ---> TODO: based on the REINFORCE script, create the actor-critic script\n",
        "training_iteration = 0\n",
        "recent_rewards = []\n",
        "\n",
        "while True:\n",
        "  state, _ = env.reset()\n",
        "\n",
        "  #save\n",
        "  #1. probabilities of chosen actions pi(actions given state)\n",
        "  #2. rewards r\n",
        "  #3. state values V from critic\n",
        "  saved_probabilities = []\n",
        "  saved_rewards = []\n",
        "  saved_values = []\n",
        "\n",
        "  for t in range(HORIZON + 1):\n",
        "    probabilities = actor(state)\n",
        "    state_value = critic(state)\n",
        "\n",
        "    distribution = Categorical(probabilities)\n",
        "    action = distribution.sample()\n",
        "\n",
        "    next_state, reward, terminated, _, _ = env.step(action.item())\n",
        "\n",
        "    saved_probabilities.append(probabilities[0][action])\n",
        "    saved_rewards.append(reward)\n",
        "    saved_values.append(state_value)\n",
        "\n",
        "    # Update state\n",
        "    state = next_state\n",
        "\n",
        "    if terminated:\n",
        "        break\n",
        "\n",
        "  advantages = []\n",
        "  returns = []\n",
        "  discounted_return = 0\n",
        "\n",
        "  #computing returns in reverse order\n",
        "  for r, v in zip(reversed(saved_rewards), reversed(saved_values)):\n",
        "    discounted_return = r + DISCOUNT_FACTOR * discounted_return\n",
        "    advantage = discounted_return - v.item()\n",
        "    returns.insert(0, discounted_return)\n",
        "    advantages.insert(0, advantage)\n",
        "\n",
        "  advantages = torch.tensor(advantages)\n",
        "  returns = torch.tensor(returns)\n",
        "\n",
        "  #Found that training was very unstable, found this stability trick online and implementing it seems to work\n",
        "  advantages = (advantages - advantages.mean())/ (advantages.std() + 1e-8)\n",
        "\n",
        "  #actor_loss computation\n",
        "  actor_loss = []\n",
        "  for prob, advantage in zip(saved_probabilities, advantages):\n",
        "    time_step_actor_loss = -advantage * torch.log(prob)\n",
        "    actor_loss.append(time_step_actor_loss)\n",
        "  actor_loss = torch.stack(actor_loss).sum()\n",
        "\n",
        "  critic_loss = []\n",
        "  for v, r in zip(saved_values, returns):\n",
        "    time_step_critic_loss = F.mse_loss(v, torch.tensor([r]))\n",
        "    critic_loss.append(time_step_critic_loss)\n",
        "  critic_loss = torch.stack(critic_loss).sum()\n",
        "\n",
        "  #update actor and critic\n",
        "  actor_optimizer.zero_grad()\n",
        "  actor_loss.backward()\n",
        "  actor_optimizer.step()\n",
        "\n",
        "  critic_optimizer.zero_grad()\n",
        "  critic_loss.backward()\n",
        "  critic_optimizer.step()\n",
        "\n",
        "  episode_total_reward = sum(saved_rewards)\n",
        "  recent_rewards.append(episode_total_reward)\n",
        "\n",
        "  log_frequency = 100\n",
        "  training_iteration += 1\n",
        "  if training_iteration % log_frequency == 0:\n",
        "    if training_iteration <= 10000:\n",
        "      # Save networks\n",
        "      torch.save(actor, actor_path)\n",
        "      torch.save(critic, critic_path)\n",
        "\n",
        "      if len(recent_rewards) > 200:\n",
        "        recent_rewards.pop(0)\n",
        "\n",
        "      # Stopping criteria\n",
        "      if (len(recent_rewards) == 200 and\n",
        "        all(r >= 400 for r in recent_rewards[-200:]) and\n",
        "        all(r >= 400 for r in recent_rewards[-100:])):\n",
        "          print(\"Achieved sustained high performance - stopping training\")\n",
        "          break\n",
        "\n",
        "      # Print results every 100 iterations\n",
        "      if training_iteration % 100 == 0:\n",
        "        print(f\"Iteration {training_iteration} - Last reward: {episode_total_reward:.2f}\")\n",
        "        print(f\"Actor loss: {actor_loss.item():.4f}, Critic loss: {critic_loss.item():.4f}\")\n",
        "      else:\n",
        "          break\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bd8015-4da9-4ea1-a5cc-edaac4587c59",
      "metadata": {
        "id": "79bd8015-4da9-4ea1-a5cc-edaac4587c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2afddd-d87f-403e-ad37-4bcb2f4edf49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActorModelV0(\n",
            "  (fc0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (policy): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-baf10532008f>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  policy = torch.load(\"./actor_3.pt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_reward = 500.0\n"
          ]
        }
      ],
      "source": [
        "# Script to test the policy after training\n",
        "\n",
        "# Create policy\n",
        "policy = ActorModelV0()\n",
        "policy.eval()\n",
        "print(policy)\n",
        "\n",
        "# Load the trained policy\n",
        "policy = torch.load(\"./actor_3.pt\")\n",
        "\n",
        "# Create environment\n",
        "env = CartpoleEnvV0()\n",
        "\n",
        "# Reset it\n",
        "total_reward = 0.0\n",
        "state, _ = env.reset(seed=None)\n",
        "\n",
        "# While the episode is not finished\n",
        "terminated = False\n",
        "while not terminated:\n",
        "\n",
        "    # Use the policy to generate the probabilities of each action\n",
        "    probabilities = policy(state)\n",
        "\n",
        "    # ---> TODO: how to select an action\n",
        "    distribution = Categorical(probabilities)\n",
        "    action = distribution.sample().item()\n",
        "\n",
        "    # One step forward\n",
        "    state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "    # Render (or not) the environment\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "\n",
        "# Print reward\n",
        "env.close()\n",
        "print(\"total_reward = {}\".format(total_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a0b7cb-bacd-4baa-b97d-26d03dfe221f",
      "metadata": {
        "id": "b5a0b7cb-bacd-4baa-b97d-26d03dfe221f"
      },
      "source": [
        "### Continuous actions (difficult)\n",
        "In this project, we considered a discrete action space.\n",
        "How could we use policy gradient methods for continuous action spaces?\n",
        "You can give your ideas, you can code them, and ultimately, you can even find an environment with a continuous action space and try to solve it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j9Zhm5B6RLZq",
      "metadata": {
        "id": "j9Zhm5B6RLZq"
      },
      "source": [
        "The result of the netowrk should be a continuous random variable. FOr example, if it were the normal distribution, the result would be 2 neurons representing the mean and variance of the normal distribution. However, normal distribution is bad for this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f8cf36a-8fd1-4d9b-a498-08444316bede",
      "metadata": {
        "id": "4f8cf36a-8fd1-4d9b-a498-08444316bede"
      },
      "source": [
        "**Your answer**: TODO."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05NSMSDPaJ_0"
      },
      "id": "05NSMSDPaJ_0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}